{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALR Client Side (notebook)\n",
    "This notebook is a copy similar to ALR_Client_Side found in:\n",
    "https://github.com/rfernand387/ALR_Earth_Engine/blob/master/ALR_Client_Side.ipynb\n",
    "\n",
    "Modifications have been made to accept an EE image with 10 m resolution bands that has been outputted from SL2P10_control.ipynb:\n",
    "https://github.com/kateharvey/Sentinel2_ALR/blob/main/shared/SL2P10_control.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=FAxalyWQnrOOIQLwl0Oux3PnU5jeI_AbfqLpe13d810&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=FAxalyWQnrOOIQLwl0Oux3PnU5jeI_AbfqLpe13d810&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AX4XfWhunKeMFxIYK1_36iCMDkg4nIbSt8ls3BpPCGcJVxhKGSIHHMUH6x4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test image using an uploaded asset for now\n",
    "inputImage = ee.Image('users/kateharvey/test_collection/20200801T155911_20200801T160644_T18TVQ_LAI')\n",
    "\n",
    "# Change variable name here to match band name pattern (one of: Albedo, fAPAR, fCOVER, LAI, CCC, CWC, DASF)\n",
    "outputName = 'LAI'\n",
    "defaultBand = 'estimate'+outputName\n",
    "\n",
    "# List the bands that we expect in the image (ensure number of bands in the list below matches the number bands in the input image)\n",
    "# This is the order of bands produced by running SL2P10 and exporting the resulting ImageCollection\n",
    "inputImage_bands = ee.List(['B2', 'B3', 'B4', 'B8', 'date', 'QC', 'estimateLAI', 'partition', 'networkID', 'errorLAI', 'partition_1', 'networkID_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegetation Indices Sources\n",
    "\n",
    "1. https://www.hindawi.com/journals/js/2017/1353691/tab1/\n",
    "2. https://www.hiphen-plant.com/blog/vegetation-indices/\n",
    "3. https://gisgeography.com/sentinel-2-bands-combinations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we define a list of strings representing the expressions for each vegetation index as a function of the bands in the input image\n",
    "# More vegetation indices can be defined, but the list CANNOT contain any two vegetation indices which are a linear combination of each\n",
    "# other or LARs will fail to select the requested number of variables\n",
    "\n",
    "# The formatting of the expression must be\n",
    "# \"<name of VI> = <expression with band names from inputImage_bands used as variables in the form b('<band name>')\"\n",
    "\n",
    "# Only include VIs that use 10 m bands (B2, B3, B4, B8)\n",
    "input_VI_definition = ee.List([\"RAW_B2  = b('B2')\",\n",
    "                               \"RAW_B3  = b('B3')\",\n",
    "                               \"RAW_B4  = b('B4')\",\n",
    "                               \"RAW_B8  = b('B8')\",\n",
    "                               \"GI      = b('B3')/b('B4')\",\n",
    "                             # \"RVI3    = b('B4')/b('B6')\",\n",
    "                             # \"SR3     = b('B5')/b('B4')\",\n",
    "                             # \"GM1     = b('B6')/b('B3')\",\n",
    "                             # \"GM2     = b('B6')/b('B5')\",\n",
    "                             # \"SR2     = b('B7')/b('B3')\",\n",
    "                             # \"PSSR    = b('B7')/b('B4')\",\n",
    "                               \"SGI     = b('B8')/b('B4')\",\n",
    "                             # \"MSI     = b('B11')/b('B7')\",\n",
    "                             # \"II      = b('B11')/b('B12')\",\n",
    "                               \"GVI     = (b('B8')/b('B3'))-1\",\n",
    "                             # \"PSRI    = (b('B4')-b('B3'))/b('B6')\",\n",
    "                               \"NDVI3   = ((b('B8')-b('B4'))/(b('B8')))+b('B4')\",\n",
    "                             # \"SR5     = 1/b('B5')\",\n",
    "                             # \"SR6     = b('B4')/(b('B3')*b('B5'))\",\n",
    "                             # \"SR7     = b('B8')/(b('B3')*b('B5'))\",\n",
    "                             # \"IPVI    = b('B7')/(b('B7')+b('B4'))\",\n",
    "                             # \"ARI     = (1/b('B3'))-(1/b('B5'))\",\n",
    "                             # \"ARI2    = b('B7')*((1/b('B3'))-(1/b('B5')))\",\n",
    "                               \"NDVI    = (b('B8')-b('B4'))/(b('B8')+b('B4'))\",\n",
    "                               \"GNDVI   = (b('B8')-b('B3'))/(b('B8')+b('B3'))\",\n",
    "                             # \"NDWI    = (b('B8')-b('B11'))/(b('B8')+b('B11'))\",\n",
    "                             # \"NDREVI  = (b('B8')-b('B5'))/(b('B8')+b('B5'))\",\n",
    "                               \"NDGI    = (b('B3')-b('B4'))/(b('B3')+b('B4'))\",\n",
    "                             # \"NDI1    = (b('B7')-b('B5'))/(b('B7')-b('B4'))\",\n",
    "                             # \"NDI2    = (b('B8')-b('B5'))/(b('B8')-b('B4'))\",\n",
    "                             # \"RENDVI  = (b('B6')-b('B5'))/(b('B6')+b('B5'))\",\n",
    "                             # \"OSAVI   = (1.16*(b('B7')-b('B4')))/(b('B7')+b('B4')+0.61)\",\n",
    "                             # \"NMDI    = (b('B8')-(b('B11')-b('B12')))/(b('B8')+(b('B11')-b('B12')))\",\n",
    "                             # \"HI      = ((b('B3')-b('B5'))/(b('B3')+b('B5')))-0.5*b('B5')\",\n",
    "                             # \"GVSP    = (-0.283*b('B3') - 0.66*b('B4') + 0.577*b('B6') + 0.388*b('B8'))/(0.433*b('B3') - 0.632*b('B4') + 0.586*b('B6') + 0.264*b('B8A'))\",\n",
    "                             # \"MCARI   = ((b('B5')-b('B4'))-0.2*(b('B5')-b('B3')))*(b('B5')/b('B4'))\",\n",
    "                             # \"TCARI   = 3*((b('B5')-b('B4'))-0.2*(b('B5')-b('B3'))*(b('B5')/b('B4')))\",\n",
    "                               \"EVI     = 2.5*((b('B8')-b('B4'))/(b('B8')+6*b('B4')-7.5*b('B3')+1))\",\n",
    "                               \"EVI2    = 2.5*((b('B8')-b('B4'))/(b('B8')+2.4*b('B4')+1))\",\n",
    "                               \"RDVI    = (b('B8')-b('B4'))/((b('B8')+b('B4'))**0.5)\",\n",
    "                               \"MSR     = ((b('B8')/b('B4'))-1)/((b('B8')/b('B4'))**0.5+1)\",\n",
    "                             # \"MSAVI   = 0.5*(2*b('B7')+1-((2*b('B7')+1)**2-8*(b('B7')-b('B4')))**0.5)\",\n",
    "                               \"MSAVI2  = 0.5*(2*b('B8')+1-((2*b('B8')+1)**2-8*(b('B8')-b('B4')))**0.5)\",\n",
    "                             # \"MCARI2  = (1.5*(2.5*(b('B7')-b('B4'))-1.3*(b('B7')-b('B3'))))/((((2*b('B7')+1)**2)-(6*b('B7')-5*(b('B4')**0.5))-0.5)**0.5)\",\n",
    "                             # \"MTVI2   = (1.5*(1.2*(b('B7')-b('B3'))-2.5*(b('B4')-b('B3'))))/(((2*b('B7')+1)**2-(6*b('B7')-5*b('B4'))-0.5)**0.5)\",\n",
    "                             # \"MSR2    = ((b('B7')/b('B4'))-1)/(((b('B7')/b('B4'))+1)**0.5)\",\n",
    "                               \"NLI     = ((b('B8')**2)-b('B4'))/((b('B8')**2)+b('B4'))\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following functions each input is recast to the expected data type as function parameter typing is not enforced in Earth Engine\n",
    "when defining functions, and later methods called on these parameters within the function must recognize the type of the parameter\n",
    "independently of other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes an image, a list of strings for the names of the bands in the image to rename to, a string which is the \n",
    "# name of the band containing the response variable in the image, and the list of strings defining vegetation indices to add to the image.\n",
    "# It returns an image which contains all of the original bands in the image renamed and all of the vegetation indices defined earlier\n",
    "# with the response band being the last band defined in the image\n",
    "def format_image(image, image_bands, response_band, VI_definition):\n",
    "    image = ee.Image(image)\n",
    "    image_bands = ee.List(image_bands)\n",
    "    response_band = ee.String(response_band)\n",
    "    VI_definition = ee.List(VI_definition)\n",
    "    \n",
    "    # image_bands specifices a list of the names of the bands used in defining the expressions for VIs in VI_definition\n",
    "    image = image.rename(image_bands).toDouble()\n",
    "    \n",
    "    # Generate an imageCollection from a list of expressions defining a set of Vegetation Indices using the bands available in the image\n",
    "    VIimageCollection = ee.ImageCollection(VI_definition.map(lambda expr: image.expression(expr)))\n",
    "    VIimage = VIimageCollection.toBands().regexpRename(\"[0-9]+_\", \"\")\n",
    "    \n",
    "    # Reorder the bands in the image so the response band is the last band in the image\n",
    "    feature_bands = image_bands.remove(response_band)\n",
    "    \n",
    "    return image.select(feature_bands).addBands(VIimage).addBands(image.select(response_band))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes an image and retrieves the total number of pixels in the image as an integer\n",
    "def get_num_pixels(image):\n",
    "    image_dimensions = ee.List(image.getInfo()[\"bands\"][28][\"dimensions\"])\n",
    "        # 28 is the index of the last band using the inputs defined above (12 input bands plus 17 VIs = 29 total bands)\n",
    "    image_height = image_dimensions.getNumber(0)\n",
    "    image_width = image_dimensions.getNumber(1)\n",
    "    image_pixels = image_height.multiply(image_width)\n",
    "    \n",
    "    return image_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes an image and a string which is the name of the band containing the response variable in the image\n",
    "# It returns an image with the response band centred to a mean 0, and the other bands in the image standardized to a mean 0 and a \n",
    "# standard deviation of 1. This preprocessing is necessary for the LARs algorithm\n",
    "def scale_image(image, response_band):\n",
    "    image = ee.Image(image)\n",
    "    response_band = ee.String(response_band)\n",
    "    image_pixels = ee.Number(get_num_pixels(image))\n",
    "    \n",
    "    # Set up lists containing the input/feature bands in the image\n",
    "    bandList = image.bandNames()\n",
    "    featureList = bandList.remove(response_band)\n",
    "    num_bands = bandList.length()\n",
    "    num_features = featureList.length()\n",
    "    \n",
    "    # We will be using the reduceRegion() function on images from Earth Engine, \n",
    "    # which will process up to a specified number of pixels from the image to generate the outputs of the reducer\n",
    "    max_pixels = image_pixels.min(10000000)\n",
    "    # best_effort = ee.Algorithms.If(image_pixels.gt(max_pixels), True, False)\n",
    "    \n",
    "    # Set default projection and scale using the response band\n",
    "    defaultScale = image.select(defaultBand).projection().nominalScale()\n",
    "    defaultCrs = image.select(defaultBand).projection().crs()\n",
    "    image = image.setDefaultProjection(crs=defaultCrs, scale=defaultScale)\n",
    "    \n",
    "    # Center all of the bands in the image for LARs, we will centre the sampled data later as well as reduceRegion() is not precise enough\n",
    "    meanImage = image.subtract(image.reduceRegion(reducer=ee.Reducer.mean(), scale=defaultScale, bestEffort=True, maxPixels=max_pixels).toImage(bandList))\n",
    "    \n",
    "    # Separate the image into features (X) and response (y) as we need to standardize the input features\n",
    "    X = meanImage.select(featureList)\n",
    "    y = meanImage.select(response_band)\n",
    "    \n",
    "    # Standardize the input features\n",
    "    X = X.divide(X.reduceRegion(reducer=ee.Reducer.stdDev(), bestEffort=True, maxPixels=max_pixels).toImage(featureList))\n",
    "    \n",
    "    return X.addBands(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function implements the LARs algorithm fully as described in (et al. 2002)\n",
    "# It takes an image, a string which is the name of the band containing the response variable in the image, the number of non-zero\n",
    "# coefficients requested for the LARs algorithm to select the best features to predict the response in the image\n",
    "# Additionally the function requires the number of samples (pixels) from the image that the user wishes to process. \n",
    "# These inputs are necessary as Earth Engine provides a limited amount of RAM (2GB) and processing time on their virtual machines,\n",
    "# so the user may need to adjust how many pixels they wish to process in the image in case the function leads to a \n",
    "# \"User memory limit exceeded error\" or \"Computation timed out error\"\n",
    "def ee_LARS(image, response_band, num_nonzero_coefficients, num_samples):\n",
    "    image = ee.Image(image)\n",
    "    response_band = ee.String(response_band)\n",
    "    num_nonzero_coefficients = ee.Number(num_nonzero_coefficients)\n",
    "    num_samples = ee.Number(num_samples)\n",
    "    image_pixels = ee.Number(get_num_pixels(image))\n",
    "\n",
    "    # Retrieve the list of features in the image by getting all of the band names and removing the response band name from the list\n",
    "    bandList = image.bandNames()\n",
    "    featureList = bandList.remove(response_band)\n",
    "    \n",
    "    # Randomly sample pixels in the image at its native resolution into a feature collection\n",
    "    # (either as many as requested in the function call, or all of the pixels in the image if the total number of pixels in the image \n",
    "    # is less than the number of samples requested) \n",
    "    inputCollection = image.sample(numPixels=num_samples.min(image_pixels))\n",
    "    n = inputCollection.size()\n",
    "    m = featureList.length()\n",
    "    \n",
    "    # Use an aggregate array function over the feature collection and map the function over each feature in the band list\n",
    "    # to generate a dictionary of all of the samples retrieved\n",
    "    inputs = ee.Dictionary.fromLists(bandList, bandList.map(lambda feature: inputCollection.aggregate_array(feature)))\n",
    "    \n",
    "    # Although we may call our scale_image function on the input image, the reduceRegion() function used to determine the mean\n",
    "    # and standard deviation of each band in the image over the entire region is not precise enough over a large image\n",
    "    # so we must recenter all of the bands in the image and now we can also normalize (L2 norm) each input feature as is required\n",
    "    # by the LARs algorithm\n",
    "    \n",
    "    # Use an aggregate_mean function over the feature collection to get the mean of each band\n",
    "    input_means = ee.Dictionary.fromLists(bandList, bandList.map(lambda feature: inputCollection.aggregate_mean(feature)))\n",
    "\n",
    "    # Center all of the bands in the image by mapping a function over the list of features and then a subtract over the list of all samples for each band\n",
    "    def centre_inputs(key, value):\n",
    "        key_mean = input_means.getNumber(key)\n",
    "        return ee.List(value).map(lambda sample: ee.Number(sample).subtract(key_mean))\n",
    "    inputs = inputs.map(centre_inputs)\n",
    "    \n",
    "    # Separate the response variable samples into its own vector\n",
    "    y = inputs.toArray([response_band]).reshape([-1,1])\n",
    "    \n",
    "    # Remove response band from the feature collection by selecting only bands in the feature list\n",
    "    inputs = inputs.select(featureList)\n",
    "    \n",
    "    # Generate a dictionary of all of the L2 norms of the input features using a custom mapped function\n",
    "    input_norms = inputs.map(lambda key, value: ee.Number(ee.List(value).map(lambda sample: ee.Number(sample).pow(2)).reduce(ee.Reducer.sum())).pow(0.5))\n",
    "    \n",
    "    # Normalize all of the features by mapping a function over the list of features and then map a division over the list of all of the samples of the feature\n",
    "    def norm_inputs(key, value):\n",
    "        key_norm = input_norms.getNumber(key)\n",
    "        return ee.List(value).map(lambda sample: ee.Number(sample).divide(key_norm))\n",
    "    inputs = inputs.map(norm_inputs)\n",
    "    \n",
    "    # Generate the array of samples using the dictionary\n",
    "    X = inputs.toArray(featureList).transpose()\n",
    "    \n",
    "    # Find the first best predictor of the response to initialize the main LARs loop\n",
    "    initial_prediction = ee.Array(ee.List.repeat([0], n))\n",
    "    c = X.transpose().matrixMultiply(y.subtract(initial_prediction))\n",
    "    c_abs = c.abs()\n",
    "    C_maxLoc = c_abs.project([0]).argmax()\n",
    "    add_feature = C_maxLoc.getNumber(0)\n",
    "    A = ee.List([add_feature])\n",
    "    \n",
    "    # Create a dicitionary of initial inputs to pass into the main LARs iterative loop\n",
    "    # The iterate function in Earth Engine processes each iteration as a tree of iterations with no access to any variables\n",
    "    # from previous iterations, only those that are passed to the next iteration, so we must pass both the current prediction and\n",
    "    # the active set of features (with non-zero coefficients), A\n",
    "    initial_inputs = ee.Dictionary({'prediction': initial_prediction, 'A': A})\n",
    "    \n",
    "    def LARs_regression(iteration, inputs):\n",
    "        inputs = ee.Dictionary(inputs)\n",
    "        \n",
    "        # Find the active set of features, A (predictors with non-zero coefficients)\n",
    "        A = ee.List(inputs.get('A'))\n",
    "        # A_list is an array used to mask the full array of input samples and the correlation vector\n",
    "        A_list = ee.Array(ee.List.sequence(0, m.subtract(1)).map(lambda index: A.contains(index)).replaceAll(False, 0).replaceAll(True, 1)).reshape([-1,1])\n",
    "        \n",
    "        # The following matrix algebra determines the next most correlated variable, or the next best predictor considering the\n",
    "        # current features in the active set, A, as well as the magnitude to adjust the prediction vector to ensure all of the features\n",
    "        # in the active set are equally correlated to response vector\n",
    "        prediction = inputs.getArray('prediction')\n",
    "        c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "        c_abs = c.abs()\n",
    "        C_max = c_abs.get(c_abs.argmax())\n",
    "        s_A = c.divide(c_abs).mask(A_list)\n",
    "        X_A = X.mask(A_list.transpose())\n",
    "        G_Ai = X_A.transpose().matrixMultiply(X_A).matrixInverse()\n",
    "        G1 = G_Ai.matrixMultiply(s_A)\n",
    "        A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "        w_A = G1.multiply(A_A)\n",
    "        u_A = X_A.matrixMultiply(w_A)\n",
    "        a = X.transpose().matrixMultiply(u_A)\n",
    "        a = a.project([0])\n",
    "        c = c.project([0])\n",
    "        \n",
    "        def compute_gammaArray(index_j):\n",
    "            minus_j = C_max.subtract(c.get([index_j])).divide(A_A.subtract(a.get([index_j])))\n",
    "            plus_j = C_max.add(c.get([index_j])).divide(A_A.add(a.get([index_j])))\n",
    "            return ee.List([minus_j, plus_j]).filter(ee.Filter.gte('item', 0)).reduce(ee.Reducer.min())\n",
    "        \n",
    "        A_c = ee.List.sequence(0, m.subtract(1)).removeAll(A)\n",
    "        gammaArray = A_c.map(compute_gammaArray)\n",
    "        gamma = gammaArray.reduce(ee.Reducer.min())\n",
    "        min_location = gammaArray.indexOf(gamma)\n",
    "        add_feature = A_c.getNumber(min_location)\n",
    "        \n",
    "        # Update our active set of variables with the next best predictor from the non-active set and update the prediction vector\n",
    "        A = A.add(add_feature)\n",
    "        prediction = prediction.add(u_A.multiply(gamma))\n",
    "        \n",
    "        return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "    \n",
    "    # For the final iteration of LARs(if selecting all of the input variables) we require a different method to determine the magnitude\n",
    "    # to adjust the magnitude of the prediction vector, as the regular LARs iteration relies on the variables in the non-active set\n",
    "    # In the final iteration there will be no variables in the non-active set, so the method will not work\n",
    "    def LARs_final_iteration(iteration, inputs):\n",
    "        inputs = ee.Dictionary(inputs)\n",
    "        A = ee.List(inputs.get('A'))\n",
    "        \n",
    "        prediction = inputs.getArray('prediction')\n",
    "        c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "        c_abs = c.abs()\n",
    "        C_max = c_abs.get(c_abs.argmax())        \n",
    "        \n",
    "        s_A = c.divide(c_abs)\n",
    "        G_Ai = X.transpose().matrixMultiply(X).matrixInverse()\n",
    "        G1 = G_Ai.matrixMultiply(s_A)\n",
    "        A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "        w_A = G1.multiply(A_A)\n",
    "        u_A = X.matrixMultiply(w_A)\n",
    "        \n",
    "        gamma = C_max.divide(A_A)\n",
    "        prediction = prediction.add(u_A.multiply(gamma))\n",
    "        \n",
    "        return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "    \n",
    "    # Actually carrying out the iterations by iterating over a placeholder list which is a sequence from 1 to the number of non-zero\n",
    "    # variables that the user wishes to select as predictors for the response\n",
    "    iterations = ee.List.sequence(1, m.subtract(1).min(num_nonzero_coefficients))\n",
    "    penultimate_outputs = iterations.iterate(LARs_regression, initial_inputs)\n",
    "    final_outputs = ee.Dictionary(ee.Algorithms.If(num_nonzero_coefficients.gte(m), LARs_final_iteration(m, penultimate_outputs), penultimate_outputs))\n",
    "    \n",
    "    final_prediction = final_outputs.getArray('prediction')\n",
    "    A = ee.List(final_outputs.get('A'))\n",
    "    feature_path = A.slice(0, num_nonzero_coefficients).map(lambda index: featureList.getString(index))\n",
    "    \n",
    "    # The code snippet below is able to extract the exact coefficients on all of the selected features, but is commented out\n",
    "    # as it adds computational complexity that takes up unnecessary memory on the Google Earth Engine virtual machine since we\n",
    "    # are only using LARs as a feature selection algorithm\n",
    "\n",
    "#     coefficients = X.matrixSolve(final_prediction).project([0]).toList().map(lambda num: ee.Algorithms.If(ee.Number(num).abs().lt(0.001), 0, num))\n",
    "#     print('Coefficients')\n",
    "#     coeff = ee.Dictionary.fromLists(featureList, coefficients).getInfo()\n",
    "#     ordered_coeff = OrderedDict()\n",
    "#     var_path = feature_path.cat(featureList.removeAll(feature_path)).getInfo()\n",
    "#     for key in var_path:\n",
    "#         ordered_coeff[key] = coeff[key]\n",
    "#     print(json.dumps(ordered_coeff, indent=1))\n",
    "\n",
    "    return feature_path #.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function trims input data according to an algorithm in which the response band is partitioned into n equally sized\n",
    "# partitions, and in each of the n partitions, for the features selected by LARs, they are each trimmed individually down to only the\n",
    "# 5-95 percentile of the data. We are not doing any preprocessing with the data, so the raw data is exported from Earth Engine\n",
    "# The function takes an image, a list of strings with the selected feature bands in the image, the string that is the name of the response\n",
    "# band in this image, the number of samples/pixels the user wants to take from the image, and the number of parititions to trim within\n",
    "def trim_data(image, selected_features, response_band, num_samples, num_partitions):\n",
    "    image = ee.Image(image)\n",
    "    selected_features = ee.List(selected_features)\n",
    "    response_band = ee.String(response_band)\n",
    "    num_samples = ee.Number(num_samples)\n",
    "    num_partitions = ee.Number(num_partitions)\n",
    "    \n",
    "    # Generate the list of percentile bounds for the requested number of partitions, and the names of the value bounds for the\n",
    "    # dictionary that will be generated from the percentile reducer used later on\n",
    "    percentiles = ee.List.sequence(0, 100, ee.Number(100).divide(num_partitions))\n",
    "    percentile_names = percentiles.map(lambda num: ee.Number(num).round().toInt().format(\"p%s\"))\n",
    "    \n",
    "    # Randomly sample the pixels in the input image into a feature collection containing only the selected features and the response\n",
    "    image_pixels = ee.Number(get_num_pixels(image))\n",
    "    inputsCollection = image.select(selected_features.add(response_band)).sample(numPixels=num_samples.min(image_pixels))\n",
    "    \n",
    "    # Find the values at the percentile bounds using the percentile reducer over the feature collection\n",
    "    response_percentiles = inputsCollection.reduceColumns(ee.Reducer.percentile(percentiles=percentiles, outputNames=percentile_names, maxRaw=inputsCollection.size()), [response_band])\n",
    "    \n",
    "    # Create a list of percentile bounds for each partition\n",
    "    response_partitions = response_percentiles.values(percentile_names.remove('p100')).zip(response_percentiles.values(percentile_names.remove('p0')))\n",
    "    \n",
    "    # We'll use the following function mapped over the response_partitions list to partition the data by the requested number of partitions\n",
    "    def partition_data(partition_range):\n",
    "        partition_range = ee.List(partition_range)\n",
    "        return inputsCollection.filter(ee.Filter.rangeContains(response_band, partition_range.getNumber(0), partition_range.getNumber(1)))\n",
    "    \n",
    "    partitioned_data = response_partitions.map(partition_data)\n",
    "    \n",
    "    # The following function now trims the data in each partition individually for each feature to its 5-95 percentile only\n",
    "    def trim_partitions(partition):\n",
    "        partition = ee.FeatureCollection(partition)\n",
    "        feature_trimming_bounds = selected_features.map(lambda feature: ee.List([feature]).cat(partition.reduceColumns(ee.Reducer.percentile([5, 95]), [feature]).values(['p5','p95'])))\n",
    "        def trimmer(current_feature, collection):\n",
    "            current_feature = ee.List(current_feature)\n",
    "            collection = ee.FeatureCollection(collection)\n",
    "            return collection.filter(ee.Filter.rangeContains(current_feature.getString(0), current_feature.getNumber(1), current_feature.getNumber(2)))\n",
    "        return feature_trimming_bounds.iterate(trimmer, partition)\n",
    "    \n",
    "    # Retrieve the trimmed data partitions and flatten the paritions into a single trimmed feature collection\n",
    "    trimmed_partitions = partitioned_data.map(trim_partitions)\n",
    "    trimmed_data = ee.FeatureCollection(trimmed_partitions).flatten()\n",
    "    \n",
    "    return trimmed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(x):\n",
    "    return x if x>=0 else (math.exp(x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return math.log(math.exp(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softsign(x):\n",
    "    return x/(abs(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(x, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (math.exp(2*x)-1)/(math.exp(2*x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nnet(inputs, keras_model):\n",
    "    activation_functions = {\n",
    "        \"elu\": elu,\n",
    "        \"softplus\": softplus,\n",
    "        \"softsign\": softsign,\n",
    "        \"relu\": relu,\n",
    "        \"tanh\": tanh,\n",
    "        \"sigmoid\": sigmoid}\n",
    "    \n",
    "    for layer in keras_model.layers:\n",
    "        layer_weights = layer.get_weights()\n",
    "        node_weights = layer_weights[0]\n",
    "        bias = layer_weights[1]\n",
    "        \n",
    "        inputs = inputs.dot(node_weights)+bias\n",
    "        \n",
    "        activation_function = layer.get_config()[\"activation\"]\n",
    "        if(activation_function != \"linear\"):\n",
    "            activation_function = activation_functions[activation_function]\n",
    "            \n",
    "            inputs = activation_function(inputs)\n",
    "            \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function exports the keras model in a way that can be parsed into a feature collection in Earth Engine and applied to images manually\n",
    "\n",
    "def export_nnet(keras_model, X):\n",
    "    nnet_data = []\n",
    "    headers = []\n",
    "    prev_layer_size = len(X.keys())\n",
    "    layer_num = 0\n",
    "    \n",
    "    for layer in keras_model.layers:\n",
    "        layer_info = layer.get_config()\n",
    "        num_nodes = layer_info[\"units\"]\n",
    "        activation_function = layer_info[\"activation\"]\n",
    "        layer_weights = layer.get_weights()[0]\n",
    "        layer_bias = layer.get_weights()[1]\n",
    "        \n",
    "        headers = list(set(headers) | set([x for x in range((prev_layer_size+1)*num_nodes)]))\n",
    "        \n",
    "        layer_data = [0, 0, layer_num, prev_layer_size, num_nodes, activation_function] + layer_weights.flatten().tolist() + layer_bias.tolist()\n",
    "        nnet_data.append(layer_data)\n",
    "        \n",
    "        prev_layer_size = num_nodes\n",
    "        layer_num += 1\n",
    "    \n",
    "    nnet_data.insert(0, [\"latitude\", \"longitude\", \"layer_num\", \"prev_layer_size\", \"num_nodes\", \"activation\"]+headers)\n",
    "    return nnet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nets(X, y):\n",
    "    LAI_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(5, input_shape=[len(X.keys())]),\n",
    "        tf.keras.layers.Dense(4, activation=\"softsign\"),\n",
    "        tf.keras.layers.Dense(3, activation=\"softsign\"),\n",
    "        tf.keras.layers.Dense(2, activation=\"softsign\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compiling the model to minimize the mean squared error loss function and use the NADAM optimizer\n",
    "    LAI_model.compile(optimizer=tf.keras.optimizers.Nadam(), loss='mse', metrics=['mse', 'mae'])\n",
    "\n",
    "    # Fitting the model to our trimmed data\n",
    "    LAI_model.fit(x=X.to_numpy(), y=y.to_numpy(), epochs=100)\n",
    "\n",
    "    return LAI_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputImage = format_image(inputImage, inputImage_bands, defaultBand, input_VI_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_pixels = get_num_pixels(inputImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledImage = scale_image(inputImage, defaultBand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select_features = ee_LARS(scaledImage, defaultBand, 5, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the trimmed data is processed then in a neural network created using tensorflow to find nonlinear relationships between the predictor and the response. Earth Engine does not have this functionality (for free) to generate neural network based models.\n",
    "\n",
    "Here we also see how the server side in the Earth Engine API is completely separate from the client side on the local machine. We need\n",
    "to export our trimmed data as a CSV to a google drive which is synced into the \"gdrive\" folder in our local machine using the \n",
    "Backup and Sync software or using google-drive-ocamlfuse on Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the export task on the server side from Earth Engine. Remember that the data will be exported to the google drive of the google\n",
    "# account you used when you initiated the Earth Engine API authentication flow, so ensure that, that accounts drive is synced to the \n",
    "# gdrive folder in the same folder as this script\n",
    "trimmedCollection = trim_data(image=inputImage.updateMask(inputImage.select(defaultBand).gt(0)),\n",
    "                                  selected_features=select_features,\n",
    "                                  response_band=defaultBand,\n",
    "                                  num_samples=50000,\n",
    "                                  num_partitions=10)\n",
    "\n",
    "exportData = ee.batch.Export.table.toDrive(collection=trimmedCollection,\n",
    "                                           description=\"image_data_samples\",\n",
    "                                           fileFormat=\"CSV\")\n",
    "\n",
    "# Starting the export data task\n",
    "exportData.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY\n",
      "RUNNING\n",
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Essentially a wait loop to see if the data has finished exporting by checking with the server-side\n",
    "prev_task_status = ee.data.getTaskStatus(exportData.id)[0][\"state\"]\n",
    "print(prev_task_status)\n",
    "while exportData.active():\n",
    "    task_status = ee.data.getTaskStatus(exportData.id)[0][\"state\"]\n",
    "    if(task_status != prev_task_status):\n",
    "        print(task_status)\n",
    "    prev_task_status = task_status\n",
    "    time.sleep(5)\n",
    "print(ee.data.getTaskStatus(exportData.id)[0][\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more robust way to check if the data has been exported properly, we can use a wait loop to check on our local machine until the exported data file exists in the synced \"gdrive\" folder\n",
    "\n",
    "From here on out, all of the processing is done using your local hardware and packages, so it may be helpful to use a powerful machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file into pandas dataframes\n",
    "trimmed_data = pd.read_csv('./gdrive/image_data_samples.csv')\n",
    "X = trimmed_data.drop(labels=['estimateLAI', 'system:index', '.geo'], axis=1)\n",
    "y = trimmed_data.estimateLAI\n",
    "\n",
    "# We preprocess the input features by standardizing them to a mean of 0 and a standard deviation of 1 for the neural network\n",
    "X = pd.DataFrame(skl.preprocessing.scale(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-25 10:59:53.747751: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-25 10:59:53.982064: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "920/920 [==============================] - 4s 3ms/step - loss: 7782361.6808 - mse: 7782361.6808 - mae: 2636.3218: 0s - loss: 7786814.3988 - \n",
      "Epoch 2/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7785807.7828 - mse: 7785807.7828 - mae: 2636.2569\n",
      "Epoch 3/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7744315.5185 - mse: 7744315.5185 - mae: 2627.9425\n",
      "Epoch 4/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7702595.5396 - mse: 7702595.5396 - mae: 2623.9628\n",
      "Epoch 5/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7697828.2595 - mse: 7697828.2595 - mae: 2621.3797\n",
      "Epoch 6/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7662342.6813 - mse: 7662342.6813 - mae: 2612.4583\n",
      "Epoch 7/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7670980.2899 - mse: 7670980.2899 - mae: 2614.0698\n",
      "Epoch 8/100\n",
      "920/920 [==============================] - 3s 4ms/step - loss: 7654812.2497 - mse: 7654812.2497 - mae: 2612.1467\n",
      "Epoch 9/100\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 7694514.9528 - mse: 7694514.9528 - mae: 2618.8893\n",
      "Epoch 10/100\n",
      "920/920 [==============================] - 3s 4ms/step - loss: 7679880.1591 - mse: 7679880.1591 - mae: 2617.1578\n",
      "Epoch 11/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7593360.2964 - mse: 7593360.2964 - mae: 2600.0991\n",
      "Epoch 12/100\n",
      "920/920 [==============================] - 3s 4ms/step - loss: 7605612.0841 - mse: 7605612.0841 - mae: 2601.5015\n",
      "Epoch 13/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7627542.0755 - mse: 7627542.0755 - mae: 2607.2761\n",
      "Epoch 14/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7590788.4506 - mse: 7590788.4506 - mae: 2600.3292\n",
      "Epoch 15/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7547399.0809 - mse: 7547399.0809 - mae: 2590.4644\n",
      "Epoch 16/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7554240.9566 - mse: 7554240.9566 - mae: 2592.7763\n",
      "Epoch 17/100\n",
      "920/920 [==============================] - 3s 4ms/step - loss: 7572352.1466 - mse: 7572352.1466 - mae: 2596.3839\n",
      "Epoch 18/100\n",
      "920/920 [==============================] - 3s 4ms/step - loss: 7528752.3230 - mse: 7528752.3230 - mae: 2587.6521\n",
      "Epoch 19/100\n",
      "920/920 [==============================] - 5s 5ms/step - loss: 7496105.3469 - mse: 7496105.3469 - mae: 2583.2356\n",
      "Epoch 20/100\n",
      "920/920 [==============================] - 5s 5ms/step - loss: 7501489.3502 - mse: 7501489.3502 - mae: 2583.3013\n",
      "Epoch 21/100\n",
      "920/920 [==============================] - 4s 5ms/step - loss: 7504185.6189 - mse: 7504185.6189 - mae: 2582.3366: 0s - loss: 7504801.7982 - mse: 7504801.7982 - mae:\n",
      "Epoch 22/100\n",
      "920/920 [==============================] - 6s 6ms/step - loss: 7494386.1699 - mse: 7494386.1699 - mae: 2579.9303\n",
      "Epoch 23/100\n",
      "920/920 [==============================] - 6s 6ms/step - loss: 7495662.5874 - mse: 7495662.5874 - mae: 2583.0494\n",
      "Epoch 24/100\n",
      "920/920 [==============================] - 7s 7ms/step - loss: 7440496.8301 - mse: 7440496.8301 - mae: 2572.6937\n",
      "Epoch 25/100\n",
      "920/920 [==============================] - 6s 6ms/step - loss: 7418627.2258 - mse: 7418627.2258 - mae: 2565.4977\n",
      "Epoch 26/100\n",
      "920/920 [==============================] - 5s 5ms/step - loss: 7404764.4262 - mse: 7404764.4262 - mae: 2564.2540\n",
      "Epoch 27/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7364515.2818 - mse: 7364515.2818 - mae: 2555.8229\n",
      "Epoch 28/100\n",
      "920/920 [==============================] - 5s 5ms/step - loss: 7390412.2258 - mse: 7390412.2258 - mae: 2562.8526\n",
      "Epoch 29/100\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 7391643.3279 - mse: 7391643.3279 - mae: 2562.0131\n",
      "Epoch 30/100\n",
      "920/920 [==============================] - 3s 4ms/step - loss: 7326808.0955 - mse: 7326808.0955 - mae: 2549.2181\n",
      "Epoch 31/100\n",
      "920/920 [==============================] - 5s 5ms/step - loss: 7308462.5098 - mse: 7308462.5098 - mae: 2547.1236\n",
      "Epoch 32/100\n",
      "920/920 [==============================] - 6s 6ms/step - loss: 7326051.1069 - mse: 7326051.1069 - mae: 2547.5983\n",
      "Epoch 33/100\n",
      "920/920 [==============================] - 6s 6ms/step - loss: 7370101.6292 - mse: 7370101.6292 - mae: 2557.8924\n",
      "Epoch 34/100\n",
      "920/920 [==============================] - 5s 6ms/step - loss: 7267220.9701 - mse: 7267220.9701 - mae: 2536.0419\n",
      "Epoch 35/100\n",
      "920/920 [==============================] - 6s 6ms/step - loss: 7260307.6618 - mse: 7260307.6618 - mae: 2538.1860: 0s - loss: 7259715.8412 - mse: 7259715.8412 - mae: 2\n",
      "Epoch 36/100\n",
      "920/920 [==============================] - ETA: 0s - loss: 7249324.0656 - mse: 7249324.0656 - mae: 2534.21 - 7s 8ms/step - loss: 7249434.9224 - mse: 7249434.9224 - mae: 2534.2353\n",
      "Epoch 37/100\n",
      "920/920 [==============================] - 7s 8ms/step - loss: 7285129.3431 - mse: 7285129.3431 - mae: 2540.2265\n",
      "Epoch 38/100\n",
      "920/920 [==============================] - 7s 8ms/step - loss: 7213615.1010 - mse: 7213615.1010 - mae: 2527.2255\n",
      "Epoch 39/100\n",
      "920/920 [==============================] - 5s 5ms/step - loss: 7317175.4685 - mse: 7317175.4685 - mae: 2547.3592\n",
      "Epoch 40/100\n",
      "920/920 [==============================] - 4s 5ms/step - loss: 7169207.0174 - mse: 7169207.0174 - mae: 2516.7609\n",
      "Epoch 41/100\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 7160443.4153 - mse: 7160443.4153 - mae: 2516.8823\n",
      "Epoch 42/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7135183.2210 - mse: 7135183.2210 - mae: 2511.2519\n",
      "Epoch 43/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7144119.5456 - mse: 7144119.5456 - mae: 2514.8696\n",
      "Epoch 44/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 7121861.6162 - mse: 7121861.6162 - mae: 2507.1958: 1s - los\n",
      "Epoch 45/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 7174292.2123 - mse: 7174292.2123 - mae: 2517.9183\n",
      "Epoch 46/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 7158639.4718 - mse: 7158639.4718 - mae: 2514.7262\n",
      "Epoch 47/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 7133975.9061 - mse: 7133975.9061 - mae: 2510.0916\n",
      "Epoch 48/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 7071533.1716 - mse: 7071533.1716 - mae: 2498.8147\n",
      "Epoch 49/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 7101110.2921 - mse: 7101110.2921 - mae: 2506.1908\n",
      "Epoch 50/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 7018395.8046 - mse: 7018395.8046 - mae: 2488.5591\n",
      "Epoch 51/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 7117130.0771 - mse: 7117130.0771 - mae: 2507.7922: 0s - loss: 7123223.6517 - mse: 7123223.6\n",
      "Epoch 52/100\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 7110831.1450 - mse: 7110831.1450 - mae: 2508.5481\n",
      "Epoch 53/100\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 7081698.4463 - mse: 7081698.4463 - mae: 2500.9783\n",
      "Epoch 54/100\n",
      "920/920 [==============================] - 6s 6ms/step - loss: 7006305.7638 - mse: 7006305.7638 - mae: 2485.9772\n",
      "Epoch 55/100\n",
      "920/920 [==============================] - 5s 6ms/step - loss: 6985175.5385 - mse: 6985175.5385 - mae: 2479.5249\n",
      "Epoch 56/100\n",
      "920/920 [==============================] - 6s 6ms/step - loss: 7024350.7731 - mse: 7024350.7731 - mae: 2489.0642\n",
      "Epoch 57/100\n",
      "920/920 [==============================] - 5s 6ms/step - loss: 7001387.9984 - mse: 7001387.9984 - mae: 2484.1915\n",
      "Epoch 58/100\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 6941896.2264 - mse: 6941896.2264 - mae: 2471.0055\n",
      "Epoch 59/100\n",
      "920/920 [==============================] - 4s 5ms/step - loss: 6954527.1602 - mse: 6954527.1602 - mae: 2474.4978\n",
      "Epoch 60/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6914153.5293 - mse: 6914153.5293 - mae: 2465.4535\n",
      "Epoch 61/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6939084.4549 - mse: 6939084.4549 - mae: 2470.6997\n",
      "Epoch 62/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6907493.0597 - mse: 6907493.0597 - mae: 2465.4368\n",
      "Epoch 63/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6921482.5554 - mse: 6921482.5554 - mae: 2467.7278\n",
      "Epoch 64/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6885665.1010 - mse: 6885665.1010 - mae: 2460.5174\n",
      "Epoch 65/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6872030.4940 - mse: 6872030.4940 - mae: 2455.8712\n",
      "Epoch 66/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6891724.3708 - mse: 6891724.3708 - mae: 2461.6028\n",
      "Epoch 67/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6792790.5608 - mse: 6792790.5608 - mae: 2443.3474\n",
      "Epoch 68/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6828763.4175 - mse: 6828763.4175 - mae: 2447.4259\n",
      "Epoch 69/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6829446.3523 - mse: 6829446.3523 - mae: 2448.2447\n",
      "Epoch 70/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6778758.6379 - mse: 6778758.6379 - mae: 2438.5384\n",
      "Epoch 71/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6787070.1238 - mse: 6787070.1238 - mae: 2440.2860\n",
      "Epoch 72/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6779100.4729 - mse: 6779100.4729 - mae: 2439.5732\n",
      "Epoch 73/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6741058.5717 - mse: 6741058.5717 - mae: 2433.3475\n",
      "Epoch 74/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6745824.7334 - mse: 6745824.7334 - mae: 2434.9037\n",
      "Epoch 75/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6721157.4750 - mse: 6721157.4750 - mae: 2426.5862: 0s - loss: 6720792.6431 - mse: 67207\n",
      "Epoch 76/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6716144.3246 - mse: 6716144.3246 - mae: 2427.2847\n",
      "Epoch 77/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6700646.0950 - mse: 6700646.0950 - mae: 2423.4774: 0s - loss: 6700390.3578 - mse: 6700390.3578 \n",
      "Epoch 78/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6639990.6933 - mse: 6639990.6933 - mae: 2411.0038\n",
      "Epoch 79/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6666890.4718 - mse: 6666890.4718 - mae: 2416.8585\n",
      "Epoch 80/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6664561.1868 - mse: 6664561.1868 - mae: 2414.4819\n",
      "Epoch 81/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 6642684.9989 - mse: 6642684.9989 - mae: 2409.8498\n",
      "Epoch 82/100\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 6649209.2573 - mse: 6649209.2573 - mae: 2412.6478\n",
      "Epoch 83/100\n",
      "920/920 [==============================] - 4s 4ms/step - loss: 6655685.2210 - mse: 6655685.2210 - mae: 2414.1469\n",
      "Epoch 84/100\n",
      "920/920 [==============================] - 3s 3ms/step - loss: 6630621.3230 - mse: 6630621.3230 - mae: 2408.4222\n",
      "Epoch 85/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6549321.2448 - mse: 6549321.2448 - mae: 2392.2081\n",
      "Epoch 86/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6626262.3702 - mse: 6626262.3702 - mae: 2407.2015\n",
      "Epoch 87/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6594466.0537 - mse: 6594466.0537 - mae: 2403.0334: 1s - loss:\n",
      "Epoch 88/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6526702.8078 - mse: 6526702.8078 - mae: 2387.6341\n",
      "Epoch 89/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6557111.7389 - mse: 6557111.7389 - mae: 2393.4533\n",
      "Epoch 90/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6507952.6732 - mse: 6507952.6732 - mae: 2383.1497\n",
      "Epoch 91/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6495451.2269 - mse: 6495451.2269 - mae: 2381.3569\n",
      "Epoch 92/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6527303.7959 - mse: 6527303.7959 - mae: 2387.1114\n",
      "Epoch 93/100\n",
      "920/920 [==============================] - 2s 2ms/step - loss: 6506555.1395 - mse: 6506555.1395 - mae: 2383.5348\n",
      "Epoch 94/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6462636.6873 - mse: 6462636.6873 - mae: 2373.5641\n",
      "Epoch 95/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6477675.9723 - mse: 6477675.9723 - mae: 2376.5215\n",
      "Epoch 96/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6446955.7128 - mse: 6446955.7128 - mae: 2370.2456\n",
      "Epoch 97/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6444957.6287 - mse: 6444957.6287 - mae: 2367.2416\n",
      "Epoch 98/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6458216.8089 - mse: 6458216.8089 - mae: 2373.8797: 1s - loss: 647\n",
      "Epoch 99/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6395489.2763 - mse: 6395489.2763 - mae: 2360.5124\n",
      "Epoch 100/100\n",
      "920/920 [==============================] - 2s 3ms/step - loss: 6379396.0782 - mse: 6379396.0782 - mae: 2355.3846\n"
     ]
    }
   ],
   "source": [
    "LAI_model = make_nets(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting our own input data to evaluate the performance (for now)\n",
    "LAI_predictions = pd.Series(LAI_model.predict(X.to_numpy()).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code the prepares the predictions to be displayed against the true values of the response for matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_tf_LAI = np.vstack([y, LAI_predictions])\n",
    "z_tf_LAI = scipy.stats.gaussian_kde(xy_tf_LAI)(xy_tf_LAI)\n",
    "\n",
    "idx_tf_LAI = z_tf_LAI.argsort()\n",
    "x_tf_LAI = y[idx_tf_LAI]\n",
    "y_tf_LAI = LAI_predictions[idx_tf_LAI]\n",
    "z_tf_LAI = z_tf_LAI[idx_tf_LAI]\n",
    "\n",
    "rmse_tf_LAI = skl.metrics.mean_squared_error(x_tf_LAI, y_tf_LAI, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAJOCAYAAACA3sJZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABDXElEQVR4nO3dd5wcdf3H8deHFHoRCYKEIiJSlCYCgkoICSQhEDqJGAKC9CqigCCKotgAkUBEBCkCUkJIYkIgHfxJCVWqVDUUE2oIBEj5/v74bvS87OX2ktubu93X8/HYR+52ZnffOze5e+/Md2YipYQkSZKqY5miA0iSJNUyy5YkSVIVWbYkSZKqyLIlSZJURZYtSZKkKrJsSZIkVZFlSypARBwVERcVnaM9iogLIuLoonNIUmuxbGmJRcRLEdFrMdMjIl6IiCfLTNs8Iu6MiLci4u2IeDAi+jWYfmZEvBgRsyNiekT8qdHj+0fE/RHxXkS8ERF/jIjui8nyg4i4rpn384OISBGxXaP7D42I+aUssyLi0Yjo32iewyPi6Yh4NyL+HRF/joiVm3idrsBZwC8a3b9i6TXGlHlM2WUdET0iYvri3leDeSdHxAel13gnIqZGxOfLzHdoaTkcWOa1UkQMbXT/PRFxaIPHLlxWs0s/w6siYuNGj1nc8voF8L3ScloipeU1p5ThtYj4Q0Ss1GD6H0rvZa9Gj7uodP/C99M1In5VWgcXvp8Lm3idhbdLKsw4pLTezyo9/88jonOD6Q1/XrMj4pkG03aIiLsi4s2ImBkRN0fE2g2mLxsRw0rL9s2IGBUR6zSR4+BG+d8vLYMvlKafFhGPl35WL0bEaY0ev0FETCo97umG62lErB0RIyPildJzbtDosb+MiGdLz/10RBzSaHqniPhx6fHvRsTDEbFahctvdqPb/Ij4TYPpB0bEU6XnfTIi9q50+S3uPZemn1BaVrMiYlpEfLncslf9sGypmr4KrAlsGBFfbDRtFHAX8InSPCcCsyD/EgUGA71SSisB2wITFj4wIvYHrgd+DawBbA58CNwTER9bkqAREaXXfBMYUmaWv5ayrAZcCtzY4Jf+zsBPgEEppZWBTYGbFvNyA4CnU0ovN7p//9L72K3hH85WdnzpfXwcmAxcW2aeITS9HN4DDmn8R7ORhctqVaAXMAd4MCI+B80vr5TSq8DTwF6Nn7iF9izl2ArYGjij0fS/0+A9lv5QHwA832CeM8jr33bAysAuwMPlXqfB7fgK860AnExeh7cHdgW+3Wie4xs872cb3P8x4HJgA2B94F3gqgbTTwK+BGwBfBJ4G/gNZaSU/tgwP3As8ALwUGmWAA4pvWYf4PiIGNjgKW4gL5OPA98DbomIbqVpC4A7gP2aWAbvAXuS15UhwK8jYscG038I7Fh6L6uQ/49+UJq22OXX6D19grwe3gxQKk7XAd8qPe9pwPURsWaFy6/J9xwR2wPnk/8/rwr8HrgtIjo1sQxUByxbqqYhwO3AGP73j9oawKeA36WUPird/pJSuqc0yxeBcSml5wFSSq+llC4vPTaAXwE/Lv2RmJNSeg04ApgNnLKEWb9C/qV6EjAwmtiqklJaQC4oKwKfaZD3rymlh0vzvJlSujql9G4Tr9UXmFLm/iHAMOAx4OAlfB8VSSnNA24ENmt4f0SsD+wMHAnsHhGfaPTQt4E/AOdU8BrzU0rPp5SOJb/fH5QmVbK8JgN7tOxdNZnjNWAcuXQ1NArYqUFB70Ne9q81mOeLwG0ppVdS9lJK6ZpWynVZSunu0vr/MvBHYKcKHzs2pXRzSmlWSul94JJGj/0U+f/Qv1NKH5B/1ptXGG0IcE0qXV4kpfTzlNJDKaV5KaVnyP+ndwKIvMVyG+Cc0v/FW4G/USpXpde/FHigifdxTkrp6ZTSgpTSfcDd5JJD6edyMvDNlNI/Ssv/8dL7aeny2x+YUXp+gO7A26XlmFJKfyYXv083t/yae8/kAvxESunB0jK8hlwIFxY51SHLlqoiIlYg/4L7Y+nWsMC8ATwHXBcRe5f5g34veevJaRGxbaNPhJ8F1qP0CXWhUgm6Fei9hJGHkP/4Ltxd2b/cTKUshwFzgX+U7r6PXEx+GBE7RcSyzbzW54FnGt4REesBPfjv8jpk0Ye1ntLP4mDysm7oEGBa6Q/IU5QvfecB+0XEZ8tMa8pwcqGFypbXU8CWLXj+JkXevdyXvM419AEwEli4leYQ8h/Ghu4FvhURx0bE50tlv9LXXS/yLvL1KnzIV4EnGt3304h4PSL+EhE9WvDY35OL5CdL/xcPBsZWkHn90nOVLZSl9/+VBq+1OfBCo6L8KJUXu4bPvTy53C587s8D84D9I+8K/ntEHLeYpyi3/Bb6nwIJTAOeioi9Srsq9yZvVX6sNH1xy6+59zwW6BQR25d+X3wDeIT/LfGqM5YtVcu+5F9edwKjgc6UtlSUfuHtArxE3kr1auTxQ58pTb8OOAHYnbxFZEZEnF563jVK/75a5jVfbTC9YqVfpgcA16eU5gK3sOgutB0i4m3yH+hfAl9PKc0o5b279H63Af4MvBF5kHdTuw1WI+/2aegQ4LGU0pPkXRSbR8TWLX0vFbi49D5mA8eTd9M0znF96evrKbMrsbSlaBhwbgte9xVg9dLjK1le75KX09IYERHvAv8ib9UotzXuGnKxX5W8RW9Eo+k/BX5G/mM7DXi5tJu78eu83eD2TYCU0j9TSqullP7ZXNCIOIy8u/KXDe7+LrAhsA55l+GoiPh0mcduAXyfvCtsob8D/wReJu+e35TKfl6HAHenlF5sYvoPyH83Fu6yXAl4p9E875B3ubbUMHJpGVf6vjt5N9zG5C1N+wM/iIhFPlA1sfwWTluP/LO9euF9KaX55J/99eTfU9cDR6WU3ivNsrjl19x7fpf8we+e0nOfAxzZoOipDlm2VC1DgJtKux4+JG/Z+M8fqZTS9JTS8SmlT5PHnLxHg0/TpV2Evch/cI8Gzo2I3YHXS7OUG9O0doPpLbEP+RP0woHpfwT6Nhh3AnBvSmk18riVkfx3K83CvGNTSnuSC8UA4FDyrs1y3mLRP0aHlF6XlNIr5JJZbszU0jqx9D6WI2+9u6X0x5qI2In8R+3G0rzXA5+PiK3KPM/PyFunKt36tA55HBhQ0fJambzLchGlgcsLBz2fuZjX3DvlMWE9gE0oU8RLu667kQ9YGJ1SmtNo+vyU0tCU0k7kdfE84MqI2LTR66zW4Pa7xWQq9372Jo/x6ZtS+s/6m1K6L6X0bkrpw5TS1cBfgH6NHrsReUvKSaUSu9Bl5J/xx8m7vIdTwZYt8np4dbkJEXF8afoepf/TkEv7Ko1mXYVFP0wsVkT8AvgccGCDUrLwZ3FuaXfdY+R1s/Ey2Jsyy6/Re7qnYYEsDWj/OXnd6EouY1c0WNcXt/yae89HkLdmbV567q8DoyPik80tB9Uuy5ZaXWm3TU/g66XN/6+RP5X2K43X+h8ppX8BQ8m/bBtPm5tSupm8ef9z5N1v08lbohq+5jLkMRMTGj9HBYaQP63+s5T1ZqALMKhMntnkAcSDy215Ko09mQBMLPd+Sh4jf1pfmH1H8vivMxosr+2BQdHg6KrWVMp5N3nX2m6lu4eQB0M/UspwX+n+RXZpppTeAC4CflThS+7Df8fLNM5RbnltSt7KUS770em/g59/0twLp5SmkMeZLbLVo+Q64FSa2HXW4HnmpJSGksvyZoubt1IR0Qf4HXmQ/d+amT2Rfz4LH7s+MB74UUqp8YEOWwJ/SHk83Ifkwd3blfv/1+D5diKPW7ylzLRvAKcDu6aUGh79+gT5AJiGHx62pOndeeVe94fk3by7pZRmNZi0cJdek1uEKlx+5QrkVsDUlNK00jr4AHl9X3hU4eKWX3PveUtgVErp76XnvoO81b3hwH/VGcuWllaXiFiuwa0z+Yihv5PHV21Vum1MLkmDIuJjpfE6G0XEMqVfYN+gNH4o8ukD9oiIlUvT+5I/Jd5X+tT7beCsiPhaRCwfEWsBV5A/XV5I05ZplHXZyEcl7UreyrMw65bkLTdltyyVisYV5F03RMSAiBhYel8R+dQRO7PoeKiFxpSmLzSEfGTmZg0yfI58tFXfZpY1pQzLNbo1O7YoIr5Ues0nImI54EDywPitGtxOAA5uovRdQP4DsmmZaQsP2/9U5MPte1DaZVnh8tqZyrbEVOoioHcTW+kuJo/1m1rmPZwc+ZQXy0dE59IuxJVZ9IjEFouInuStmfullO5vNG21iNh94c85Ig4mj0kaV5q+DrmgDk0pDSvz9A9Q2j0aEV3IHxBeaWLLz0JDgFsbjUWi9No/AXqnlF5oOC2l9HfyeKRzSln3IR/Bd2uDxy8HLByXt2zp+4XTzgC+VnruNxo99/Pkgv690v/VTYGDyMMSFrv8Gjz/juStqjc3mvQA8JWF60Ppg9NX+G/Ba3L5VfCeHwD2iIgNS+t3b/Lvv8fLZVSdSCl587ZEN/KYq9To9mPyYfsnlJn/O+RxLyuSP2m+RN4k/xp5nNI6pfn2Je8yeYs8XuJvwKGNnmsA+Zfae+TdUzcA6y4m6w/KZJ1O/rT+YJn5P0keBP858i6uexpN704ej7EF+Y/gBPIuzHfJRfM7i8nShTwe5JPkXRVvkT+ZN57vUuCWZpZ1jzL3J2CjMs83mTzmbHbp9hxwSmnaQPKn7y6NHrNc6X31L73W9DI/07Tw51NaVvNLz/8e+SCCq4FNGzxmscuLvDt4OtB1KdfNXo3uu4xcJiBv6fpxE4+9p8H7OQp4kDwm523gfqB/o9eZ02CZziYfvQj5QI7ZwHpNvM4k8u7rho8dW5rWjbx+v1t63XvJhWThY88pLfeGj53dYPrHyUVkRunx9wDbNZj+BHBwo5/z2+QtV41zvkj+v9DwtYY1mL5Bad2aQ97y3Hi5L7J+Npr2YaPnPrPB9HXIp46YTT4dxVGVLL8G8/wWuLaJ5X88+f/Au6XnPrUFy6/J90ze+ngu+f/4u+SDPQYv6brsrTZukZJj9qS2FhFHApullE4uOkt7ExG/Ap5P+ZQBktThWbYkSZKqqOIxW6XxFw9HxOgy0yIiLo6I5yLisYjYpnVjSpIkdUwtGSB/Ennfczl9yUdTfYY8wPaypcwlSZJUEyoqW5EP5d+DfARWOQMonZ03pXQvsFpU79pukiRJHUal5/C5iHzUUVNnBV6HfJbmhaaX7vufs3yXBgUfCbDiiit+YZNNNmlJVkmSpEI8+OCDr6eUujU/56KaLVsR0R+YkVJ6MJq+Nle5c/osMvI+5YsJXw6w7bbbpmnTplWeVJIkqSAR8Y/m5yqvkt2IOwF7RcRL5Esl9IyI6xrNMx1Yt8H33cnXQpMkSaprzZatlNIZKaXuKaUNyCc+nJhS+nqj2UaSz7YbEbED8E5KqdyFgiVJkurKEl93LSKOBkj5UhFjyBcHfQ54HzisVdJJkiR1cC0qWymlyeRLFCwsWQvvT8BxrRlMkiSpFnghakmSpCqybEmSJFWRZUuSJKmKLFuSJElVZNmSJEmqIsuWJElSFVm2JEmSqsiyJUmSVEWWLUmSpCqybEmSJFWRZUuSJKmKLFuSJElVZNmSJEmqIsuWJElSFVm2JEmSqsiyJUmSVEWWLUmSpCqybEmSJFWRZUuSJKmKLFuSJElVZNmSJEmqIsuWJElSFVm2JEmSqsiyJUmSVEWWLUmSpCqybEmSJFWRZUuSJKmKLFuSJElVZNmSJEmqIsuWJElSFVm2JEmSqsiyJUmSVEWWLUmSpCqybEmSJFWRZUuSJKmKLFuSJElVZNmSJEmqIsuWJElSFVm2JEmSqsiyJUmSVEWWLUmSpCqybEmSJFWRZUuSJKmKLFuSJElVZNmSJEmqIsuWJElSFVm2JEmSqsiyJUmSVEWWLUmSpCqybEmSJFWRZUuSJKmKLFuSJElVZNmSJEmqIsuWJElSFVm2JEmSqsiyJUmSVEWWLUmSpCqybEmSJFWRZUuSJKmKmi1bEbFcRNwfEY9GxBMR8cMy8/SIiHci4pHS7fvViStJktSxdK5gng+Bniml2RHRBbgnIsamlO5tNN/dKaX+rR9RkiSp42q2bKWUEjC79G2X0i1VM5QkSVKtqGjMVkR0iohHgBnAXSml+8rM9qXSrsaxEbF5E89zZERMi4hpM2fOXPLUkiRJHURFZSulND+ltBXQHdguIj7XaJaHgPVTSlsCvwFGNPE8l6eUtk0pbdutW7clTy1JktRBtOhoxJTS28BkoE+j+2ellGaXvh4DdImINVopoyRJUodVydGI3SJitdLXywO9gKcbzbNWRETp6+1Kz/tGq6eVJEnqYCo5GnFt4OqI6EQuUTellEZHxNEAKaVhwP7AMRExD5gDDCwNrJckSaprlRyN+BiwdZn7hzX4+hLgktaNJkmS1PF5BnlJkqQqsmxJkiRVkWVLkiSpiixbkiRJVWTZkiRJqiLLliRJUhVZtiRJkqrIsiVJklRFli1JkqQqsmxJkiRVkWVLkiSpiixbkiRJVWTZkiRJqiLLliRJUhVZtiRJkqrIsiVJklRFli1JkqQqsmxJkiRVkWVLkiSpiixbkiRJVWTZkiRJqiLLliRJUhVZtiRJkqrIsiVJklRFli1JkqQqsmxJkiRVkWVLkiSpiixbkiRJVWTZkiRJqiLLliRJUhVZtiRJkqrIsiVJklRFli1JkqQqsmxJkiRVkWVLkiSpiixbkiRJVWTZkiRJqiLLliRJUhVZtiRJkqrIsiVJklRFli1JkqQqsmxJkiRVkWVLkiSpiixbkiRJVWTZkiRJqiLLliRJUhVZtiRJkqrIsiVJklRFli1JkqQqsmxJkiRVkWVLkiSpiixbkiRJVWTZkiRJqiLLliRJUhVZtiRJkqrIsiVJklRFli1JkqQqsmxJkiRVkWVLkiSpipotWxGxXETcHxGPRsQTEfHDMvNERFwcEc9FxGMRsU114kqSJHUsnSuY50OgZ0ppdkR0Ae6JiLEppXsbzNMX+Ezptj1wWelfSZKkutbslq2UzS5926V0S41mGwBcU5r3XmC1iFi7daNKktRK3n8f9tsP7r23+XmlpVTRmK2I6BQRjwAzgLtSSvc1mmUd4F8Nvp9euq/x8xwZEdMiYtrMmTOXMLIkSUth9mzYYw+47TZ49tmi06gOVFS2UkrzU0pbAd2B7SLic41miXIPK/M8l6eUtk0pbdutW7cWh5UkaanMmgV9+sDUqXDddTB4cNGJVAdadDRiSultYDLQp9Gk6cC6Db7vDryyNMEkSWpVb78Nu+0G990HN94IX/ta0YlUJyo5GrFbRKxW+np5oBfwdKPZRgKHlI5K3AF4J6X0amuHlSRpibz5JvTqBQ89BLfcAgccUHQi1ZFKjkZcG7g6IjqRy9lNKaXREXE0QEppGDAG6Ac8B7wPHFalvJIktczMmbloPfMMjBgB/foVnUh1ptmylVJ6DNi6zP3DGnydgONaN5okSUvptddg113hhRdg1Cjo3bvoRKpDlWzZkiSp43n5ZejZE6ZPhzFjYJddik6kOmXZkiTVnn/+MxetGTNg3Dj48peLTqQ6ZtmSJNWWF1/MReutt+Cuu2B7L2iiYlm2JEm149lnc9F6/32YMAG+8IWiE0mWLUlSjXjqqTwYfu5cmDgRttyy6EQSYNmSJNWCxx/PRSsCJk+GzTcvOpH0Hy06g7wkSe3OI49Ajx7QuTNMmWLRUrtj2ZIkdVzTpuUxWiuskIvWZz9bdCJpEZYtSVLH9Ne/5l2Hq66aLyy90UZFJ5LKsmxJkjqeqVPzRaXXXDN/vcEGRSeSmmTZkiR1LBMnQt++0L173nW47rpFJ5IWy7IlSeo4xo2DPfaADTfMRx1+8pNFJ5KaZdmSJHUMo0fDXnvBJpvApEnwiU8UnUiqiGVLktT+3XYb7LsvbLFFPjP8GmsUnUiqmGVLktS+/elPcMAB+dI748fD6qsXnUhqEcuWJKn9uu46+NrXYMcd4c4782kepA7GsiVJap+uvBIOOSSfHX7sWFh55aITSUvEsiVJan+GDYPDD4fevfPA+BVXLDqRtMQsW5Kk9uXii+GYY/IpHm6/HZZfvuhE0lKxbEmS2o9f/hJOOgn22QeGD4fllis6kbTULFuSpPbhvPPgtNPgoIPyEYhduxadSGoVli1JUrFSgnPOgbPOgsGD8xGIXboUnUpqNZ2LDiBJqmMpwRlnwM9+Bt/4Blx+OXTqVHQqqVVZtiRJxUgJTj0VLrwQjj4ahg6FZdzhotrjWi1JansLFsAJJ+SideKJcOmlFi3VLNdsSVLbWrAAjjoqb8k67TS46CKIKDqVVDWWLUlS25k/P4/NuuKKPCD+Zz+zaKnmOWZLktQ25s3Ll9+54QY491w4++yiE0ltwrIlSaq+uXNh0CC49VY4/3z47neLTiS1GcuWJKm6PvwQDjwQRo6ECy6AU04pOpHUpixbkqTqmTMH9tsPxo7NA+KPPbboRFKbs2xJkqrj/fdhwACYMAF+9zs44oiiE0mFsGxJklrf7NnQvz/cfTdcdRUMGVJ0Iqkwli1JUuuaNQv69YN7783XORw0qOhEUqEsW5Kk1vPWW9CnDzz0EPzpT3m8llTnLFuSpNbxxhvQuzc88UQ+xcNeexWdSGoXLFuSpKU3Ywb06gV//zuMGAF9+xadSGo3LFuSpKXz6quw667w0kswenQuXZL+w7IlSVpyL78MPXvmf8eOhZ13LjqR1O5YtiRJS+Yf/8hFa+ZMGDcOdtqp6ERSu2TZkiS13AsvwC67wDvvwPjxsN12RSeS2i3LliSpZZ59NhetOXNg4kTYZpuiE0ntmmVLklS5p57Kuw7nz4dJk2CLLYpOJLV7yxQdQJLUQfztb/8dAD95skVLqpBlS5LUvIcfzrsOu3aFKVNgs82KTiR1GJYtSdLi3X9/3nW44oq5aG28cdGJpA7FsiVJatr//V8+SenHPgZTp8KnP110IqnDsWxJksqbOhV22w3WWit/vf76RSeSOiTLliRpURMmQJ8+sN56eddh9+5FJ5I6LMuWJOl/3XEH9O8PG22Ujzpce+2iE0kdmmVLkvRfo0bBgAGwySb5hKVrrll0IqnDs2xJkrJbb4V994Utt8xFa401ik4k1QTLliQJbrwRDjooX+Pwrrvy0YeSWoVlS5Lq3TXXwMEHw0475fFaq65adCKppli2JKme/f73cOih+ezwY8bAyisXnUiqOZYtSapXl10GRxwBu++eB8avuGLRiaSaZNmSpHr061/DscfCnnvCiBGw/PJFJ5JqlmVLkurNz38OJ58M++0Ht9wCyy5bdCKpplm2JKme/OhH8N3vwsCB+QjErl2LTiTVvGbLVkSsGxGTIuKpiHgiIk4qM0+PiHgnIh4p3b5fnbiSpCWSEpx9Nnz/+zB4MFx3HXTuXHQqqS5U8j9tHnBqSumhiFgZeDAi7kopPdlovrtTSv1bP6IkaamkBKefnncfHn44/Pa30KlT0amkutHslq2U0qsppYdKX78LPAWsU+1gkqRWkBKcckouWsccA5dfbtGS2liLxmxFxAbA1sB9ZSZ/KSIejYixEbF5E48/MiKmRcS0mTNntjytJKlyCxbAccflIw9PPhmGDoVlHKortbWK/9dFxErArcDJKaVZjSY/BKyfUtoS+A0wotxzpJQuTyltm1Latlu3bksYWZLUrPnz4cgj87m0vvMduOACiCg6lVSXKipbEdGFXLT+mFIa3nh6SmlWSml26esxQJeI8AqmklSE+fPhsMPy2eHPPhvOP9+iJRWokqMRA/g98FRK6YIm5lmrNB8RsV3ped9ozaCSpArMnQtf/zpce20+zcO551q0pIJVcjTiTsBg4G8R8UjpvjOB9QBSSsOA/YFjImIeMAcYmFJKrR9XktSkjz6CQYNg+PA8IP6004pOJIkKylZK6R5gsR+LUkqXAJe0VihJUgt9+CEccEC+xuFFF8FJi5wSUVJBPKOdJHV0c+bAPvvAuHFw6aX5FA+S2g3LliR1ZO+9B3vtBZMmwRVX5JOWSmpXLFuS1FG9+y707w/33ANXX50vwyOp3bFsSVJH9M470Lcv3H8/XH89HHRQ0YkkNcGyJUkdzVtvwe67w8MPw003wb77Fp1I0mJYtiSpI3n9dejdG558Mp/iYc89i04kqRmWLUnqKGbMgF694Nln4fbboU+fohNJqoBlS5I6gldfhV13hZdegtGj89eSOgTLliS1d9OnQ8+euXDdcQd89atFJ5LUApYtSWrPXnopF6033sgnLd1xx6ITSWohy5YktVfPP5+L1qxZMH48fPGLRSeStAQsW5LUHj3zTB6X9cEHMHEibL110YkkLSHLliS1N08+mbdopZQvw/P5zxedSNJSWKboAJKkBh57DHr0gAiYPNmiJdUAy5YktRcPPQS77AJdu8KUKbDppkUnktQKLFuS1B7cd1/edbjyyjB1Kmy8cdGJJLUSy5YkFe0vf8mX4Pn4x/MWrQ03LDqRpFZk2ZKkIk2enC8qvfbaeYvW+usXnUhSK7NsSVJRxo+Hfv1ywZoyBdZZp+hEkqrAsiVJRRg7Fvr3h402yqd3WGutohNJqhLLliS1tZEjYe+9YfPNc9Fac82iE0mqIsuWJLWlW26B/faDrbaCCRPyoHhJNc2yJUlt5frrYeBA2H57uOsuWG21ohNJagOWLUlqC1dfDV//Onz5y3DHHbDKKkUnktRGLFuSVG2/+x0cdli+sPSYMbDSSkUnktSGLFuSVE1Dh8KRR0KfPjBqFKywQtGJJLUxy5YkVcuFF8Lxx8OAAXDbbbDcckUnklQAy5YkVcP558O3vgX77w833wzLLlt0IkkFsWxJUms791w44wwYNAhuuAG6dCk6kaQCWbYkqbWkBGedBeecA0OGwLXXQufORaeSVDB/C0hSa0gJvvMd+OUv4ZvfhGHDYBk/z0pyy5YkLb2U4OSTc9E67jiLlqT/4ZYtSVoaCxbAscfCb38Lp5wCv/oVRBSdSlI74kcvSVpS8+fDEUfkonX66RYtSWVZtiRpScybB4ceClddlQfE/+QnFi1JZbkbUZJaau7cfJ3Dm26C886DM88sOpGkdsyyJUkt8dFHMHBgPiP8L34B3/520YkktXOWLUmq1Acf5DPC//nP8Otfw4knFp1IUgdg2ZKkSsyZA3vvDXfemU/tcNRRRSeS1EFYtiSpOe+9B3vuCZMnw5VXwmGHFZ1IUgdi2ZKkxXn3XdhjD/jLX+Caa/LAeElqAcuWJDXl7behb1944IF8QekDDyw6kaQOyLIlSeW8+Sbsvjs8+ijcfDPss0/RiSR1UJYtSWrs9dehd2948kkYPhz69y86kaQOzLIlSQ39+9+w667w/PMwahTstlvRiSR1cJYtSVrolVdy0frnP/O5tHr2LDqRpBpg2ZIkgH/9K5er116DO+6Ar3yl6ESSaoRlS5Jeegl22SUPir/zTvjSl4pOJKmGWLYk1bfnnstbtGbPhgkTYNtti04kqcZYtiTVr6efzmO0PvoIJk6ErbYqOpGkGmTZklSfHn8cevWClGDSJPjc54pOJKlGLVN0AElqc48+msdoLbMMTJli0ZJUVZYtSfXlwQdz0VpuuVy0Ntmk6ESSapxlS1L9uPfePEZr1VVh6lT4zGeKTiSpDli2JNWHe+7Jl+BZY428RetTnyo6kaQ6YdmSVPsmTcoXlV5nnVy01luv6ESS6ohlS1Jtu/NO6NcPNtgAJk/OhUuS2pBlS1LtGjMG9toLPvvZXLTWWqvoRJLqkGVLUm0aMQL23juf1mHiROjWrehEkupUs2UrItaNiEkR8VREPBERJ5WZJyLi4oh4LiIei4htqhNXkipw881wwAGwzTYwfjysvnrRiSTVsUq2bM0DTk0pbQrsABwXEZs1mqcv8JnS7UjgslZNKUmV+uMfYeBA2GGHPF5rtdWKTiSpzjVbtlJKr6aUHip9/S7wFNB4hOkA4JqU3QusFhFrt3paSVqcP/wBBg+GnXeGsWNhlVWKTiRJLRuzFREbAFsD9zWatA7wrwbfT2fRQkZEHBkR0yJi2syZM1sYVZIW4/LL4bDD8vUOR4+GlVYqOpEkAS0oWxGxEnArcHJKaVbjyWUekha5I6XLU0rbppS27eZgVUmt5ZJL4KijYI89YORIWGGFohNJ0n9UVLYiogu5aP0xpTS8zCzTgXUbfN8deGXp40lSM371KzjhhHzk4fDh+ZqHktSOVHI0YgC/B55KKV3QxGwjgUNKRyXuALyTUnq1FXNK0qJ++lP49rfzkYc33QRduxadSJIW0bmCeXYCBgN/i4hHSvedCawHkFIaBowB+gHPAe8Dh7V6UklaKCU491z4wQ/g4IPzwPjOlfw6k6S21+xvp5TSPZQfk9VwngQc11qhJKlJKcH3vpe3ah16KFxxBXTqVHQqSWqSHwUldRwp5d2GF1wARx4Jl10Gy3ghDEntm2VLUseQEpx4Yj7y8Pjj4eKLIRa70V2S2gU/Ekpq/xYsgKOPzkXr1FMtWpI6FMuWpPZt/nw4/PB80tIzz4Rf/MKiJalDcTeipPZr3jwYMgSuvx5++EM4+2yLlqQOx7IlqX2aOzef1uHmm+EnP4Ezzig6kSQtEcuWpPbnww/hoIPg9tvzGeK/9a2iE0nSErNsSWpfPvgA9tsPxoyB3/wmH3koSR2YZUtS+/H++/kah+PHw29/m8+lJUkdnGVLUvswezbsuSdMmQJXXpnPDi9JNcCyJal4s2ZBv37w17/CtdfmgfGSVCMsW5KK9fbb0KcPPPgg3HgjHHBA0YkkqVVZtiQV5803Ybfd4LHH4JZbYMCAohNJUquzbEkqxsyZ0KsXPPMMjBiRdyNKUg2ybElqe6+9BrvuCi+8AKNGQe/eRSeSpKqxbElqWy+/DD17wvTp+Vxau+xSdCJJqirLlqS2889/5qI1YwaMGwdf/nLRiSSp6ixbktrGiy/movXWW3DXXbD99kUnkqQ2YdmSVH3PPpuL1vvvw4QJ8IUvFJ1IktqMZUtSdT31VB4MP3cuTJwIW25ZdCJJalOWLUnV8/jjuWhFwOTJsPnmRSeSpDa3TNEBJNWoRx6BHj2gc+d8vUOLlqQ6ZdmS1PqmTctjtFZYIRetz3626ESSVBjLlqTW9de/5l2Hq64KU6fCRhsVnUiSCmXZktR67r47X+twzTVz0dpgg6ITSVLhLFuSWsfEidCnD3Tvnncdrrtu0YkkqV2wbElaeuPGwR57wIYb5qMOP/nJohNJUrth2ZK0dEaPhr32gk02gUmT4BOfKDqRJLUrli1JS+6222DffWGLLfKZ4ddYo+hEktTuWLYkLZk//QkOOCBfemf8eFh99aITSVK7ZNmS1HLXXQdf+xrsuCPceWc+zYMkqSzLlqSWufJKOOSQfHb4sWNh5ZWLTiRJ7ZplS1Llhg2Dww+H3r3zwPgVVyw6kSS1e5YtSZW5+GI45ph8iofbb4flly86kSR1CJYtSc375S/hpJNgn31g+HBYbrmiE0lSh2HZkrR4550Hp50GBx2Uj0Ds2rXoRJLUoVi2JJWXEpxzDpx1FgwenI9A7NKl6FSS1OF0LjqApHYoJTjjDPjZz+Ab34DLL4dOnYpOJUkdkmVL0v9KCU49FS68EI4+GoYOhWXcCC5JS8rfoJL+a8ECOOGEXLROPBEuvdSiJUlLyd+ikrIFC+Coo/KWrNNOg4sugoiiU0lSh2fZkgTz5+exWVdcAd/7Xh6rZdGSpFbhmC2p3s2bly+/c8MNcO65cPbZRSeSpJpi2ZLq2dy5+YLSt9wC558P3/1u0YkkqeZYtqR69eGHcOCBMHIkXHABnHJK0YkkqSZZtqR6NGcO7LcfjB2bB8Qfe2zRiSSpZlm2pHrz/vswYABMmAC/+x0ccUTRiSSpplm2pHoyezb07w933w1XXQVDhhSdSJJqnmVLqhezZkG/fnDvvfk6h4MGFZ1IkuqCZUuqB2+9BX36wEMPwZ/+lMdrSZLahGVLqnVvvAG9e8MTT8Ctt8JeexWdSJLqimVLqmUzZkCvXvD3v8OIEdC3b9GJJKnuWLakWvXqq7DrrvDSSzB6dC5dkqQ2Z9mSatHLL0PPnvnfsWNh552LTiRJdcuyJdWaf/wjF62ZM2HcONhpp6ITSVJds2xJteSFF2CXXeCdd2D8eNhuu6ITSVLds2xJteLZZ3PRmjMHJk6EbbYpOpEkCcuWVBueeirvOpw/HyZNgi22KDqRJKlkmaIDSFpKf/vbfwfAT55s0ZKkdqbZshURV0bEjIh4vInpPSLinYh4pHT7fuvHlFTWww/nXYddu8KUKbDZZkUnkiQ1UsluxD8AlwDXLGaeu1NK/VslkaTK3H8/7L47rLJKHqP16U8XnUiSVEazW7ZSSlOBN9sgi6RK/d//5ZOUfuxjMHWqRUuS2rHWGrP1pYh4NCLGRsTmTc0UEUdGxLSImDZz5sxWemmpzkydCrvtBmutlb9ef/2iE0mSFqM1ytZDwPoppS2B3wAjmpoxpXR5SmnblNK23bp1a4WXlurMhAnQpw+st14eo9W9e9GJJEnNWOqylVKalVKaXfp6DNAlItZY6mSS/tcdd0D//rDRRvmow7XXLjqRJKkCS122ImKtiIjS19uVnvONpX1eSQ2MGgUDBsAmm+TB8GuuWXQiSVKFmj0aMSJuAHoAa0TEdOAcoAtASmkYsD9wTETMA+YAA1NKqWqJpXozfDgcdBBsvXW+1uHHPlZ0IklSCzRbtlJKg5qZfgn51BCSWtuNN8LXvw7bbw9jxsCqqxadSJLUQp5BXmqvrrkGDj4Ydtopj9eyaElSh2TZktqj3/8eDj00nx1+zBhYeeWiE0mSlpBlS2pvLrsMjjginx1+1ChYccWiE0mSloJlS2pPfv1rOPZY2HNPGDECll++6ESSpKVk2ZLai5//HE4+GfbbD265BZZdtuhEkqRWYNmS2oMf/Qi++10YODAfgdi1a9GJJEmtxLIlFSklOPts+P73YfBguO466NzsGVkkSR2IZUsqSkpw+unw4x/D4YfDVVdBp05Fp5IktTI/QktFSAlOOSUPiD/mGLjkEljGzz6SVIv87S61tQUL4LjjctE6+WQYOtSiJUk1zN/wUluaPx+OPDKfS+s734ELLoB8HXdJUo2ybEltZf58OOywfHb4s8+G88+3aElSHXDMltQW5s6FQw7Jp3X40Y/grLOKTiRJaiOWLanaPvoIBg2C4cPziUtPO63oRJKkNmTZkqrpww/hgAPyNQ4vughOOqnoRJKkNmbZkqplzhzYZx8YNw4uvTSf4kGSVHcsW1I1vPce7LUXTJoEV1yRT1oqSapLli2ptb37LvTvD/fcA1dfnS/DI0mqW5YtqTW98w707Qv33w/XXw8HHVR0IklSwSxbUmt56y3YfXd4+GG46SbYd9+iE0mS2gHLltQaXn8deveGJ5/Mp3jYc8+iE0mS2gnLlrS0ZsyAXr3g2Wfh9tuhT5+iE0mS2hHLlrQ0Xn0Vdt0VXnoJRo/OX0uS1IBlS1pS06dDz565cN1xB3z1q0UnkiS1Q5YtaUm89FIuWm+8kU9auuOORSeSJLVTli2ppZ5/PhetWbNg/Hj44heLTiRJascsW1JLPPNMHpf1wQcwcSJsvXXRiSRJ7ZxlS6rUk0/mLVop5cvwfP7zRSeSJHUAyxQdQOoQHnsMevSACJg82aIlSaqYZUtqzkMPwS67QNeuMGUKbLpp0YkkSR2IZUtanPvvz2O0Vl4Zpk6FjTcuOpEkqYOxbElN+ctf8pnhV189b9HacMOiE0mSOiDLllTO5Mn5otJrr523aK2/ftGJJEkdlGVLamz8eOjXLxesKVNgnXWKTiRJ6sAsW1JDY8dC//6w0Ub59A5rrVV0IklSB2fZkhYaORL23hs23zwXrTXXLDqRJKkGWLYkgFtugf32g622ggkT4OMfLzqRJKlGWLak66+HgQNhu+3grrtgtdWKTiRJqiGWLdW3q6+GwYPhy1+GceNglVWKTiRJqjGWLdWvK66Aww7L1zscMwZWWqnoRJKkGmTZUn0aOhS++U3o0wdGjYIVVig6kSSpRlm2VH8uvBCOPx4GDIDbboPllis6kSSphlm2VF/OPx++9S3Yf3+4+WZYdtmiE0mSapxlS/Xj3HPhjDNg0CC44Qbo0qXoRJKkOmDZUu1LCc46C845B4YMgWuvhc6di04lSaoT/sVRbUsJvvMd+OUv84D4YcNgGT9jSJLajn91VLtSgpNPzkXruOMsWpKkQviXR7VpwQI49li4+GI45RT4zW8sWpKkQvjXR7Vn/vz/7jI8/XT41a8gouhUkqQ6ZdlSbZk3Dw49FK68Mg+I/8lPLFqSpEI5QF61Y+5c+PrX4aab4Lzz4Mwzi04kSZJlSzXio49g4MB8Rvhf/AK+/e2iE0mSBFi2VAs++CCfEf7Pf4Zf/xpOPLHoRJIk/YdlSx3bnDmw995w5515QPxRRxWdSJKk/2HZUsf13nuw554weXIeEH/YYUUnkiRpEZYtdUzvvgt77AF/+Qtcc00eGC9JUjtk2VLH8/bb0LcvPPBAvqD0gQcWnUiSpCZZttSxvPkm7L47PPoo3Hwz7LNP0YkkSVosy5Y6jtdfh9694cknYfhw6N+/6ESSJDWr2TPIR8SVETEjIh5vYnpExMUR8VxEPBYR27R+TNW9f/8bevSAp5+GUaMsWpKkDqOSy/X8AeizmOl9gc+UbkcCly19LKmBV17JRevFF/O5tHbbrehEkiRVrNmylVKaCry5mFkGANek7F5gtYhYu7UCqs7961+w884wfTrccQf07Fl0IkmSWqQ1LkS9DvCvBt9PL923iIg4MiKmRcS0mTNntsJLq6a99FIuWjNm5JOWfuUrRSeSJKnFWqNsRZn7UrkZU0qXp5S2TSlt261bt1Z4adWs556Dr341n+ZhwgT40peKTiRJ0hJpjaMRpwPrNvi+O/BKKzyv6tXTT8Ouu+aLS0+cCFttVXQiSZKWWGts2RoJHFI6KnEH4J2U0qut8LyqR48/ngfDz5sHkyZZtCRJHV6zW7Yi4gagB7BGREwHzgG6AKSUhgFjgH7Ac8D7gBeo05J59FHo1Qu6dMlbtDbZpOhEkiQttWbLVkppUDPTE3BcqyVSfXrwwXzC0hVXzEXrM58pOpEkSa2iNXYjSkvn3nvzGK1VV4WpUy1akqSaYtlSse65J2/RWmMNmDIFPvWpohNJktSqLFsqzuTJ+aLS66yTi9Z66xWdSJKkVmfZUjHuugv69YMNNsila52y58GVJKnDs2yp7Y0ZA3vuCRtvnIvWWmsVnUiSpKqxbKltjRgBe+8Nn/tcPurQKwlIkmqcZUtt5+ab4YADYJttYPx4WH31ohNJklR1li21jT/+EQYOhB12yBeVXm21ohNJktQmLFuqvj/8AQYPhp13hrFjYZVVik4kSVKbsWypui6/HA47LF+GZ/RoWGmlohNJktSmLFuqnksugaOOgj32gJEjYYUVik4kSVKbs2ypOi64AE44IR95OHw4LLdc0YkkSSqEZUut76c/hVNPzUce3nQTdO1adCJJkgpj2VLrSQl++EM480w4+GC4/nro0qXoVJIkFapz0QFUI1KC730vb9U69FC44gro1KnoVJIkFc6ypaWXEnz723mc1pFHwmWXwTJuNJUkCSxbWlopwYkn5iMPjz8eLr4YIopOJUlSu+HmBy25BQvg6KNz0Tr1VIuWJEllWLa0ZObPh8MPzyctPfNM+MUvLFqSJJXhbkS13Lx5MGRIPtrwhz+Es8+2aEmS1ATLllpm7tx8Woebb4af/ATOOKPoRJIktWuWLVXuww/hoIPg9tvhV7+Cb32r6ESSJLV7li1V5oMPYL/9YMwY+M1v8pGHkiSpWZYtNe/99/M1DsePh9/+Np9LS5IkVcSypcWbPRv23BOmTIErr8xnh5ckSRWzbKlps2ZBv37w17/CtdfmgfGSJKlFLFsq7+23oU8fePBBuPFGOOCAohNJktQhWba0qDffhN12g8ceg1tugQEDik4kSVKHZdnS/5o5E3r1gmeegREj8m5ESZK0xCxb+q/XXoNdd4UXXoBRo6B376ITSZLU4Vm2lL38MvTsCdOn53Np7bJL0YkkSaoJli3BP/+Zi9aMGTBuHHz5y0UnkiSpZli26t2LL+ai9dZbcNddsP32RSeSJKmmWLbq2bPP5qL13nswYQJ84QtFJ5IkqeZYturV00/nojV3LkyaBFtuWXQiSZJqkmWrHj3+eD7qMAImT4bNNy86kSRJNWuZogOojT3yCPToAZ075+sdWrQkSaoqy1Y9mTYt7zpcYYVctD772aITSZJU8yxb9eKvf827DlddFaZOhY02KjqRJEl1wbJVD+6+O1/rcM01c9HaYIOiE0mSVDcsW7Vu4kTo0we6d8+7Dtddt+hEkiTVFctWLRs3DvbYAzbcMB91+MlPFp1IkqS6Y9mqVaNHw157wSab5PNofeITRSeSJKkuWbZq0W23wb77whZb5DPDr7FG0YkkSapblq1ac9NNcMAB+dI748fD6qsXnUiSpLpm2aol110HgwbBjjvCnXfm0zxIkqRCWbZqxZVXwiGH5LPDjx0LK69cdCJJkoRlqzYMGwaHHw69e+eB8SuuWHQiSZJUYtnq6C6+GI45Jp/i4fbbYfnli04kSZIasGx1ZL/8JZx0EuyzDwwfDsstV3QiSZLUiGWrozrvPDjtNDjoIPjTn6Br16ITSZKkMixbHU1KcM45cNZZMHhwPgKxS5eiU0mSpCZ0LjqAWiAlOPNMOP98+MY34PLLoVOnolNJkqTFsGx1FCnBqafChRfC0UfD0KGwjBsmJUlq7/xr3REsWAAnnJCL1oknwqWXWrQkSeog/Ivd3i1YAEcdlbdknXYaXHQRRBSdSpIkVciy1Z7Nn5/HZl1xBXzve/Czn1m0JEnqYByz1V7Nm5cvv3PDDXDuuXD22UUnkiRJS8Cy1R7NnQtf+xrccks+8vC73y06kSRJWkIV7UaMiD4R8UxEPBcRp5eZ3iMi3omIR0q377d+1Drx4Yew//65aF1wgUVLkqQOrtktWxHRCRgK9AamAw9ExMiU0pONZr07pdS/Chnrx5w5sN9+MHZsHhB/7LFFJ5IkSUupki1b2wHPpZReSCl9BNwIDKhurDr0/vuw115wxx3wu99ZtCRJqhGVlK11gH81+H566b7GvhQRj0bE2IjYvNwTRcSRETEtIqbNnDlzCeLWqNmzoV8/mDgRrroKjjii6ESSJKmVVFK2yp1rIDX6/iFg/ZTSlsBvgBHlniildHlKaduU0rbdunVrUdCaNWsW9OkD99yTr3M4ZEjRiSRJUiuqpGxNB9Zt8H134JWGM6SUZqWUZpe+HgN0iYg1Wi1lrXrrLejdG+67D/70Jxg0qOhEkiSplVVSth4APhMRn4qIrsBAYGTDGSJirYh8ts2I2K70vG+0dtia8sYbsOuu8PDDcOuteWC8JEmqOc0ejZhSmhcRxwPjgE7AlSmlJyLi6NL0YcD+wDERMQ+YAwxMKTXe1aiFZsyAXr3g73+H22+Hvn2LTiRJkqokiupE2267bZo2bVohr12oV1/NRevFF2HkyPy1JElq1yLiwZTStkvyWM8g35Zefhl69sz/jh0LO+9cdCJJklRllq228o9/5KI1cyaMGwc77VR0IkmS1AYsW23hhRdgl13gnXdg/HjYbruiE0mSpDZi2aq2Z5/NRWvOnHzS0m22KTqRJElqQ5atanrqqbzrcP58mDQJttii6ESSJKmNVXKeLS2Jv/3tvwPgJ0+2aEmSVKcsW9Xw8MN512HXrjBlCmy2WdGJJElSQSxbre2BB/KuwxVXzEVr442LTiRJkgpk2WpN//d/+SSlH/sYTJ0Kn/500YkkSVLBLFutZepU2G03+MQn8tfrr190IkmS1A5YtlrDhAnQpw+st17eddi9e9GJJElSO2HZWlp33AH9+8NGG+WjDtdeu+hEkiSpHbFsLY1Ro2DAANhkk3zC0jXXLDqRJElqZyxbS2r4cNh3X9hyy1y01lij6ESSJKkdsmwtiRtvhAMPzNc4vOuufPShJElSGZatlrrmGjj4YNhppzxea9VVi04kSZLaMctWS/z+93Doofns8GPGwMorF51IkiS1c5atSl12GRxxBOy+ex4Yv+KKRSeSJEkdgGWrEr/+NRx7LOy5J4wYAcsvX3QiSZLUQVi2mvPzn8PJJ8N++8Ett8CyyxadSJIkdSCWrcX50Y/gu9+FgQPzEYhduxadSJIkdTCWrXJSgrPPhu9/HwYPhuuug86di04lSZI6IMtWYynB6afDj38Mhx8OV10FnToVnUqSJHVQbq5pKCU45ZQ8IP6YY+CSS2AZ+6gkSVpyNomFFiyA447LRevkk2HoUIuWJElaarYJgPnz4cgj87m0vvMduOACiCg6lSRJqgGWrfnz4bDD8tnhzz4bzj/foiVJklpNfY/ZmjsXDjkkn9bhRz+Cs84qOpEkSaox9Vu2PvoIBg2C4cPziUtPO63oRJIkqQbVZ9n68EM44IB8jcOLLoKTTio6kSRJqlH1V7bmzIF994U77oBLL82neJAkSaqS+ipb770HAwbAxIlwxRX5pKWSJElVVD9l6913oX9/uOceuPrqfBkeSZKkKquPsvXOO9C3L9x/P1x/PRx0UNGJJElSnaj9svXWW7D77vDww3DTTXm8liRJUhup7bL1+uvQuzc8+WQ+xcOeexadSJIk1ZnaLVszZkCvXvDss3D77dCnT9GJJElSHarNsvXqq7DrrvDSSzB6dP5akiSpALVXtqZPh549c+G64w746leLTiRJkupYbZWtl17KReuNN2DcONhxx6ITSZKkOlc7Zev553PRmjULxo+HL36x6ESSJEk1UraeeSaPy/rgg3x2+K23LjqRJEkSUAtl68kn8xatlGDSJPj854tOJEmS9B/LFB1gqTz2GPToAREwebJFS5IktTsdt2w99BDssgt07QpTpsCmmxadSJIkaREds2zdf38eo7XyyjB1Kmy8cdGJJEmSyup4Zesvf8lnhl999bxFa8MNi04kSZLUpI5VtiZPzheVXnvtvEVr/fWLTiRJkrRYHadsjR8P/frlgjVlCqyzTtGJJEmSmtUxytbYsdC/P2y0UT69w1prFZ1IkiSpIu2/bI0cCXvvDZtvnovWmmsWnUiSJKli7bts3XIL7LcfbLUVTJgAH/940YkkSZJapP2Wreuvh4EDYbvt4K67YLXVik4kSZLUYu2zbF19NQweDF/+MowbB6usUnQiSZKkJdL+ytYVV8Bhh+XrHY4ZAyutVHQiSZKkJda+ytbQofDNb0KfPjBqFKywQtGJJEmSlkr7KVsXXgjHHw8DBsBtt8FyyxWdSJIkaam1j7J1/vnwrW/B/vvDzTfDsssWnUiSJKlVFF+2zj0XzjgDBg2CG26ALl2KTiRJktRqKipbEdEnIp6JiOci4vQy0yMiLi5Nfywitqno1c86C845B4YMgWuvhc6dWxhfkiSpfWu2bEVEJ2Ao0BfYDBgUEZs1mq0v8JnS7UjgsmZfefp0OO+8PCD+yiuhU6eWZpckSWr3KtmytR3wXErphZTSR8CNwIBG8wwArknZvcBqEbH2Yp/13/+G446DYcNgmeL3ZkqSJFVDJfvt1gH+1eD76cD2FcyzDvBqw5ki4kjyli+AD2Po0McZOrRFgevAGsDrRYdoh1wui3KZlOdyKc/lUp7LZVEuk/I+u6QPrKRsRZn70hLMQ0rpcuBygIiYllLatoLXrysul/JcLotymZTncinP5VKey2VRLpPyImLakj62kv1304F1G3zfHXhlCeaRJEmqO5WUrQeAz0TEpyKiKzAQGNlonpHAIaWjEncA3kkpvdr4iSRJkupNs7sRU0rzIuJ4YBzQCbgypfRERBxdmj4MGAP0A54D3gcOq+C1L1/i1LXN5VKey2VRLpPyXC7luVzKc7ksymVS3hIvl0hpkaFVkiRJaiWec0GSJKmKLFuSJElVVPWyVbVL/XRwFSyXHhHxTkQ8Urp9v4icbSkiroyIGRHxeBPT63VdaW651OO6sm5ETIqIpyLiiYg4qcw8dbe+VLhc6mp9iYjlIuL+iHi0tEx+WGaeelxXKlkudbWuNBQRnSLi4YgYXWZay9eXlFLVbuQB9c8DGwJdgUeBzRrN0w8YSz5X1w7AfdXM1B5uFS6XHsDoorO28XL5KrAN8HgT0+tuXalwudTjurI2sE3p65WBv/u7peLlUlfrS+nnv1Lp6y7AfcAOrisVLZe6WlcavfdvAdeXe/9Lsr5Ue8tWdS710/FVslzqTkppKvDmYmapx3WlkuVSd1JKr6aUHip9/S7wFPmqFQ3V3fpS4XKpK6Wf/+zSt11Kt8ZHhtXjulLJcqlLEdEd2AO4oolZWry+VLtsNXUZn5bOU2sqfc9fKm3iHRsRm7dNtHatHteVStXtuhIRGwBbkz+ZN1TX68tilgvU2fpS2iX0CDADuCul5LpCRcsF6mxdKbkI+A6woInpLV5fql22Wu1SPzWmkvf8ELB+SmlL4DfAiGqH6gDqcV2pRN2uKxGxEnArcHJKaVbjyWUeUhfrSzPLpe7Wl5TS/JTSVuSrm2wXEZ9rNEtdrisVLJe6W1cioj8wI6X04OJmK3PfYteXapctL/VTXrPvOaU0a+Em3pTSGKBLRKzRdhHbpXpcV5pVr+tKRHQhF4o/ppSGl5mlLteX5pZLva4vACmlt4HJQJ9Gk+pyXVmoqeVSp+vKTsBeEfESeYhPz4i4rtE8LV5fql22vNRPec0ul4hYKyKi9PV25J/VG22etH2px3WlWfW4rpTe7++Bp1JKFzQxW92tL5Usl3pbXyKiW0SsVvp6eaAX8HSj2epxXWl2udTbugKQUjojpdQ9pbQB+W/zxJTS1xvN1uL1pdnL9SyNVL1L/XRoFS6X/YFjImIeMAcYmEqHQdSqiLiBfPTLGhExHTiHPGizbtcVqGi51N26Qv70ORj4W2nMCcCZwHpQ1+tLJcul3taXtYGrI6ITuSzclFIaXe9/h6hsudTbutKkpV1fvFyPJElSFXkGeUmSpCqybEmSJFWRZUuSJKmKLFuSJElVZNmSJEmqIsuWJElSFVm2JEmSquj/AYr7HT2I9dnZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_tf_LAI = np.linspace(0, 3, 1000)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "\n",
    "ax.scatter(x_tf_LAI, y_tf_LAI, c=z_tf_LAI)\n",
    "ax.plot(a_tf_LAI, a_tf_LAI, c='r')\n",
    "ax.set_xlim(0,4)\n",
    "ax.set_ylim(0,4)\n",
    "ax.title.set_text('LASSO LARS (ALL BANDS) - RMSE: {}'.format(rmse_tf_LAI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early attempts to use the output of the kerasModel.get_weights() to apply the neural network weights manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-3.6639605 , -3.0396655 ,  3.2015846 , -3.2570944 , -3.664683  ],\n",
       "        [ 3.3617117 ,  3.5149114 , -4.1303396 ,  3.5270166 ,  4.0211806 ],\n",
       "        [ 0.27832007, -0.4874226 ,  0.6368207 ,  0.3136016 , -0.48899677],\n",
       "        [ 6.091278  ,  6.475897  , -6.1250844 ,  6.1704497 ,  6.391355  ],\n",
       "        [-0.40176743, -0.32247105,  1.3606182 , -1.145884  , -1.3970339 ]],\n",
       "       dtype=float32),\n",
       " array([ 63.58718 ,  62.631927, -63.65397 ,  65.68505 ,  61.51555 ],\n",
       "       dtype=float32),\n",
       " array([[-61.067574, -57.069145, -59.93873 , -60.87313 ],\n",
       "        [-61.335526, -58.730423, -60.183273, -60.649117],\n",
       "        [ 60.018684,  60.68872 ,  59.75562 ,  60.9961  ],\n",
       "        [-59.784058, -58.63574 , -57.763035, -58.15637 ],\n",
       "        [-57.8206  , -55.42128 , -58.217754, -57.664158]], dtype=float32),\n",
       " array([-20.497093, -19.560503, -20.267921, -22.241888], dtype=float32),\n",
       " array([[79.11305 , 81.26177 , 78.32159 ],\n",
       "        [79.761795, 80.95898 , 80.26317 ],\n",
       "        [80.8103  , 82.71173 , 78.384995],\n",
       "        [80.12015 , 81.995056, 78.457954]], dtype=float32),\n",
       " array([-76.33357 , -79.791115, -75.189285], dtype=float32),\n",
       " array([[-87.02893 , -88.157524],\n",
       "        [-86.62128 , -87.11389 ],\n",
       "        [-87.44118 , -88.28029 ]], dtype=float32),\n",
       " array([85.00651, 85.91   ], dtype=float32),\n",
       " array([[93.13635],\n",
       "        [92.69844]], dtype=float32),\n",
       " array([91.69858], dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAI_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu = np.vectorize(elu)\n",
    "softplus = np.vectorize(softplus)\n",
    "softsign = np.vectorize(softsign)\n",
    "relu = np.vectorize(relu)\n",
    "tanh = np.vectorize(tanh)\n",
    "sigmoid = np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276.99954\n",
      "276.99953129199605\n"
     ]
    }
   ],
   "source": [
    "row = 4802\n",
    "inputs = X.iloc[row, :].to_numpy()\n",
    "print(LAI_model.predict(inputs.reshape((-1,5)))[0][0])\n",
    "print(apply_nnet(inputs, LAI_model)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the neural network to a CSV file to be uploaded to the server side on Google Earth Engine\n",
    "export_data = export_nnet(LAI_model, X)\n",
    "with open('nnet.csv', 'w', newline='') as csvfile:\n",
    "    nnet_writer = csv.writer(csvfile)\n",
    "    for layerdata in export_data:\n",
    "        nnet_writer.writerow(layerdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, I have not found a way to automatically upload a file as an asset into Google Earth Engine from a local script. There is a command in the Earth Engine command line interface that allows you to upload an asset from cloud storage, although cloud storage is not free. There may be a workaround way to upload the file from Google Drive directly to Earth Engine, but even that would use the Google Drive API which is a part of Google Cloud, so it will not be free. It seems the network must be uploaded manually for now."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LAI_FAPAR_FCOVER_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
