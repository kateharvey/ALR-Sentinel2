{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALR Client Side\n",
    "\n",
    "This notebook is a copy similar to ALR_Client_Side found in:\n",
    "https://github.com/rfernand387/ALR_Earth_Engine/blob/master/ALR_Client_Side.ipynb\n",
    "\n",
    "Modifications have been made to accept an EE image with 10 m resolution bands that has been outputted from SL2P10_control.ipynb:\n",
    "https://github.com/kateharvey/Sentinel2_ALR/blob/main/shared/SL2P10_control.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import tensorflow as tf\n",
    "import ee\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# import custom module\n",
    "# import ALR_functions as alr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=qCXGVYsoBB8N77Sxc8k8wJRmU1I_2Qiel815trOq2H4&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=qCXGVYsoBB8N77Sxc8k8wJRmU1I_2Qiel815trOq2H4&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AX4XfWj4xh06LundIfgbSBkkHAbMBn01eT5MuxmeHsJ1LhIj8c4kdGPXHS8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def elu(x):\n",
    "    if x>=0:\n",
    "        return x\n",
    "    else:\n",
    "        return (math.exp(x)-1)\n",
    "\n",
    "\n",
    "def softplus(x):\n",
    "    return math.log(math.exp(x)+1)\n",
    "\n",
    "\n",
    "def softsign(x):\n",
    "    return x/(abs(x)+1)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return max(x, 0.0)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return (math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Functions for client-side implementation of ALR:\n",
    "# ------------------------------------------------\n",
    "# The following function takes an image, a list of the names to rename the bands to, a string which is the name of the band\n",
    "# containing the response variable in the image, and the list of strings defining vegetation indices to add to the image.\n",
    "# It returns an image which contains all of the original bands in the image renamed and all of the VIs defined earlier\n",
    "# with the response band being the last band defined in the image\n",
    "\n",
    "def format_image(image, image_bands, response_band, VI_definition):\n",
    "    image = ee.Image(image)\n",
    "    image_bands = ee.List(image_bands)\n",
    "    response_band = ee.String(response_band)\n",
    "    VI_definition = ee.List(VI_definition)\n",
    "    \n",
    "    # image_bands specifices a list of the names of the bands used in defining the expressions for VIs in VI_definition\n",
    "    image = image.rename(image_bands).toDouble()\n",
    "    \n",
    "    # Generate an ImageCollection from a list of expressions defining a set of VIs using the bands available in the image\n",
    "    VIimageCollection = ee.ImageCollection(VI_definition.map(lambda expr: image.expression(expr)))\n",
    "    VIimage = VIimageCollection.toBands().regexpRename(\"[0-9]+_\", \"\")\n",
    "    \n",
    "    # Reorder the bands in the image so the response band is the first band in the image\n",
    "    feature_bands = image_bands.remove(response_band)\n",
    "    return ee.Image(image.select(response_band).addBands(VIimage).addBands(image.select(feature_bands)))\n",
    "\n",
    "\n",
    "# The following function takes an image and retrieves the total number of pixels in the image as an integer\n",
    "\n",
    "def get_num_pixels(image):\n",
    "    \n",
    "    # get image height\n",
    "    def get_height(image):\n",
    "        height = image.getInfo()[\"bands\"][0][\"dimensions\"][0]\n",
    "        return height\n",
    "    \n",
    "    # get image width\n",
    "    def get_width(image):\n",
    "        width = image.getInfo()[\"bands\"][0][\"dimensions\"][1]\n",
    "        return width\n",
    "    \n",
    "    image_height = get_height(image)\n",
    "    image_width = get_width(image)\n",
    "    image_pixels = image_height*image_width\n",
    "    \n",
    "    return image_pixels\n",
    "\n",
    "\n",
    "# The following takes an image and the name of the band containing the response variable in the image\n",
    "# It returns an image with the response band centred to a mean 0, and the other bands in the image standardized\n",
    "# to a mean 0 and standard deviation 1. This preprocessing is necessary for the LARs algorithm\n",
    "\n",
    "def scale_image(image, response_band):\n",
    "    image = ee.Image(image)\n",
    "    response_band = ee.String(response_band)\n",
    "    image_pixels = ee.Number(get_num_pixels(image))\n",
    "    \n",
    "    # Set up lists containing the input/feature bands in the image\n",
    "    bandList = image.bandNames()\n",
    "    featureList = bandList.remove(response_band)\n",
    "    num_bands = bandList.length()\n",
    "    num_features = featureList.length()\n",
    "    \n",
    "    # We will be using the reduceRegion() function on images from Earth Engine, \n",
    "    # which will process up to a specified number of pixels from the image to generate the outputs of the reducer\n",
    "    max_pixels = image_pixels.min(10000000)\n",
    "    # best_effort = ee.Algorithms.If(image_pixels.gt(max_pixels), True, False)\n",
    "    \n",
    "    # Set default projection and scale using the response band\n",
    "    defaultScale = image.select(response_band).projection().nominalScale()\n",
    "    defaultCrs = image.select(response_band).projection().crs()\n",
    "    image = image.setDefaultProjection(crs=defaultCrs, scale=defaultScale)\n",
    "    \n",
    "    # Center all of the bands in the image for LARs\n",
    "    # We will centre the sampled data later as well as reduceRegion() is not precise enough\n",
    "    meanImage = image.subtract(image.reduceRegion(reducer=ee.Reducer.mean(), \\\n",
    "                                scale=defaultScale, bestEffort=True, maxPixels=max_pixels).toImage(bandList))\n",
    "    \n",
    "    # Separate the image into features (X) and response (y) as we need to standardize the input features\n",
    "    X = meanImage.select(featureList)\n",
    "    y = meanImage.select(response_band)\n",
    "    \n",
    "    # Standardize the input features\n",
    "    X = X.divide(X.reduceRegion(reducer=ee.Reducer.stdDev(), bestEffort=True, maxPixels=max_pixels).toImage(featureList))\n",
    "    \n",
    "    return X.addBands(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# EE LARS implementation:\n",
    "# -----------------------\n",
    "# The following function implements the LARs algorithm fully as described in (et al. 2002)\n",
    "# It takes an image, the name of the band containing the response variable in the image, the number of non-zero\n",
    "# coefficients requested for the LARs algorithm to select the best features to predict the response in the image\n",
    "# Additionally the function requires the number of samples (pixels) from the image that the user wishes to process. \n",
    "# These inputs are necessary as Earth Engine provides a limited amount of RAM (2GB) and processing time on their VMs,\n",
    "# so the user may need to adjust how many pixels they wish to process in the image in case the function leads to a \n",
    "# \"User memory limit exceeded error\" or \"Computation timed out error\"\n",
    "\n",
    "def ee_LARS(input_image, input_bandNames, response_bandName, num_nonzero_coefficients, num_samples):\n",
    "    image = ee.Image(input_image)\n",
    "    feature_list = ee.List(input_bandNames)\n",
    "    response_band = ee.String(response_bandName)\n",
    "    full_band_list = ee.List(feature_list).add(response_band)\n",
    "    num_nonzero_coefficients = ee.Number(num_nonzero_coefficients)\n",
    "    num_samples = ee.Number(num_samples)\n",
    "    image_pixels = ee.Number(get_num_pixels(image))\n",
    "    \n",
    "    # Randomly sample pixels in the image at native resolution into a FeatureCollection\n",
    "    input_collection = image.sample(numPixels=num_samples.min(image_pixels))\n",
    "    n = input_collection.size()\n",
    "    m = feature_list.length()\n",
    "    \n",
    "    # Use an aggregate array function over the FeatureCollection and map the function over each feature in the band list\n",
    "    # to generate a dictionary of all of the samples retrieved\n",
    "    inputs = ee.Dictionary.fromLists(full_band_list, full_band_list.map(lambda feature: input_collection.aggregate_array(feature)))\n",
    "    # print(inputs.getInfo())\n",
    "    \n",
    "    # Although we may call our scale_image function on the input image, the reduceRegion() function used to determine the mean\n",
    "    # and standard deviation of each band in the image over the entire region is not precise enough over a large image\n",
    "    # so we must recenter all of the bands in the image and now we can also normalize (L2 norm) each input feature as required\n",
    "    # by the LARs algorithm\n",
    "    \n",
    "    # Use an aggregate_mean function over the feature collection to get the mean of each band\n",
    "    input_means = ee.Dictionary.fromLists(full_band_list, full_band_list.map(lambda feature: input_collection.aggregate_mean(feature)))\n",
    "    \n",
    "    def centre_inputs(key, value):\n",
    "        key_mean = input_means.getNumber(key)\n",
    "        return ee.List(value).map(lambda sample: ee.Number(sample).subtract(key_mean))\n",
    "    \n",
    "    \n",
    "    # Center bands by mapping over the list of features and then a subtracting over the list of samples for each band\n",
    "    inputs = inputs.map(centre_inputs)\n",
    "\n",
    "    # Separate the response variable samples into its own vector\n",
    "    y = inputs.toArray([response_band]).reshape([-1,1])\n",
    "\n",
    "    # Remove response band from the feature collection by selecting only bands in the feature list\n",
    "    inputs = inputs.select(feature_list)\n",
    "    \n",
    "    # Generate a dictionary of all of the L2 norms of the input features using a custom mapped function\n",
    "    input_norms = inputs.map(lambda key, value: ee.Number(ee.List(value).map(lambda sample: ee.Number(sample).pow(2)).reduce(ee.Reducer.sum())).pow(0.5))\n",
    "    \n",
    "    # Normalize all of the features by mapping a function over the list of features\n",
    "    # and then map a division over the list of all of the samples of the feature\n",
    "    def norm_inputs(key, value):\n",
    "        key_norm = input_norms.getNumber(key)\n",
    "        return ee.List(value).map(lambda sample: ee.Number(sample).divide(key_norm))\n",
    "    \n",
    "    inputs = inputs.map(norm_inputs)\n",
    "    \n",
    "    # Generate the array of samples using the dictionary\n",
    "    X = inputs.toArray(feature_list).transpose()\n",
    "\n",
    "    # Find the first best predictor of the response to initialize the main LARs loop\n",
    "    initial_prediction = ee.Array(ee.List.repeat([0], n))\n",
    "    c = X.transpose().matrixMultiply(y.subtract(initial_prediction))\n",
    "    c_abs = c.abs()\n",
    "    C_maxLoc = c_abs.project([0]).argmax()\n",
    "    add_feature = C_maxLoc.getNumber(0)\n",
    "    A = ee.List([add_feature])\n",
    "    \n",
    "    # Create a dicitionary of initial inputs to pass into the main LARs iterative loop\n",
    "    # The iterate function in Earth Engine processes each iteration as a tree of iterations with no access to any variables\n",
    "    # from previous iterations (only those that are passed to the next iteration)\n",
    "    # so we must pass both the current prediction and the active set of features (with non-zero coefficients), A\n",
    "    initial_inputs = ee.Dictionary({'prediction': initial_prediction, 'A': A})\n",
    "\n",
    "    def LARs_regression(iteration, inputs):\n",
    "        inputs = ee.Dictionary(inputs)\n",
    "\n",
    "        # Find the active set of features, A (predictors with non-zero coefficients)\n",
    "        A = ee.List(inputs.get('A'))\n",
    "        # A_list is an array used to mask the full array of input samples and the correlation vector\n",
    "        A_list = ee.Array(ee.List.sequence(0, m.subtract(1))\\\n",
    "                          .map(lambda index: A.contains(index)).replaceAll(False, 0).replaceAll(True, 1)).reshape([-1,1])\n",
    "\n",
    "        # The following matrix algebra determines the next most correlated variable, or the next best predictor considering the\n",
    "        # current features in the active set, A, as well as the magnitude to adjust the prediction vector to ensure all of the\n",
    "        # features in the active set are equally correlated to response vector\n",
    "        prediction = inputs.getArray('prediction')\n",
    "        c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "        c_abs = c.abs()\n",
    "        C_max = c_abs.get(c_abs.argmax())\n",
    "        s_A = c.divide(c_abs).mask(A_list)\n",
    "        X_A = X.mask(A_list.transpose())\n",
    "        G_Ai = X_A.transpose().matrixMultiply(X_A).matrixInverse()\n",
    "        G1 = G_Ai.matrixMultiply(s_A)\n",
    "        A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "        w_A = G1.multiply(A_A)\n",
    "        u_A = X_A.matrixMultiply(w_A)\n",
    "        a = X.transpose().matrixMultiply(u_A)\n",
    "        a = a.project([0])\n",
    "        c = c.project([0])\n",
    "\n",
    "        def compute_gammaArray(index_j):\n",
    "            minus_j = C_max.subtract(c.get([index_j])).divide(A_A.subtract(a.get([index_j])))\n",
    "            plus_j = C_max.add(c.get([index_j])).divide(A_A.add(a.get([index_j])))\n",
    "            return ee.List([minus_j, plus_j]).filter(ee.Filter.gte('item', 0)).reduce(ee.Reducer.min())\n",
    "\n",
    "        A_c = ee.List.sequence(0, m.subtract(1)).removeAll(A)\n",
    "        gammaArray = A_c.map(compute_gammaArray)\n",
    "        gamma = gammaArray.reduce(ee.Reducer.min())\n",
    "        min_location = gammaArray.indexOf(gamma)\n",
    "        add_feature = A_c.getNumber(min_location)\n",
    "\n",
    "        # Update active set of variables with next best predictor from non-active set and update prediction vector\n",
    "        A = A.add(add_feature)\n",
    "        prediction = prediction.add(u_A.multiply(gamma))\n",
    "\n",
    "        return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "\n",
    "\n",
    "    # The final iteration of LARs (if selecting all input variables) requires a different method to determine magnitude for\n",
    "    # adjusting the magnitude of the prediction vector, as the regular LARs iteration relies on variables in non-active set\n",
    "    # In the final iteration there will be no variables in the non-active set, so the method will not work\n",
    "    def LARs_final_iteration(iteration, inputs):\n",
    "        inputs = ee.Dictionary(inputs)\n",
    "        A = ee.List(inputs.get('A'))\n",
    "\n",
    "        prediction = inputs.getArray('prediction')\n",
    "        c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "        c_abs = c.abs()\n",
    "        C_max = c_abs.get(c_abs.argmax())        \n",
    "\n",
    "        s_A = c.divide(c_abs)\n",
    "        G_Ai = X.transpose().matrixMultiply(X).matrixInverse()\n",
    "        G1 = G_Ai.matrixMultiply(s_A)\n",
    "        A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "        w_A = G1.multiply(A_A)\n",
    "        u_A = X.matrixMultiply(w_A)\n",
    "\n",
    "        gamma = C_max.divide(A_A)\n",
    "        prediction = prediction.add(u_A.multiply(gamma))\n",
    "\n",
    "        return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "\n",
    "    # Actually carrying out the iterations by iterating over a placeholder list (sequence from 1 to the number of non-zero\n",
    "    # variables that the user wishes to select as predictors for the response)\n",
    "    iterations = ee.List.sequence(1, m.subtract(1).min(num_nonzero_coefficients))\n",
    "    print(iterations.getInfo())\n",
    "    \n",
    "    penultimate_outputs = iterations.iterate(LARs_regression, initial_inputs)\n",
    "    \n",
    "    final_outputs = ee.Dictionary(ee.Algorithms\\\n",
    "                    .If(num_nonzero_coefficients.gte(m), LARs_final_iteration(m, penultimate_outputs), penultimate_outputs))\n",
    "    \n",
    "    final_prediction = final_outputs.getArray('prediction')\n",
    "    \n",
    "    A = ee.List(final_outputs.get('A'))\n",
    "\n",
    "    feature_path = A.slice(0, num_nonzero_coefficients).map(lambda index: feature_list.getString(index))\n",
    "    \n",
    "    # The code snippet below is able to extract the exact coefficients on all of the selected features, but is commented out\n",
    "    # as it adds computational complexity that takes up unnecessary memory on the Google Earth Engine virtual machine since we\n",
    "    # are only using LARs as a feature selection algorithm\n",
    "\n",
    "#     coefficients = X.matrixSolve(final_prediction).project([0])\\\n",
    "#                               .toList().map(lambda num: ee.Algorithms.If(ee.Number(num).abs().lt(0.001), 0, num))\n",
    "#     print('Coefficients')\n",
    "#     coeff = ee.Dictionary.fromLists(featureList, coefficients).getInfo()\n",
    "#     ordered_coeff = OrderedDict()\n",
    "#     var_path = feature_path.cat(featureList.removeAll(feature_path)).getInfo()\n",
    "#     for key in var_path:\n",
    "#         ordered_coeff[key] = coeff[key]\n",
    "#     print(json.dumps(ordered_coeff, indent=1))\n",
    "\n",
    "    return feature_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Trim data function:\n",
    "# -------------------\n",
    "# The following trims input data according to an algorithm in which the response band is partitioned into n equally sized\n",
    "# partitions, and each of the features selected by LARs are trimmed individually down to keep only the 5-95 percentile data\n",
    "# We are not doing any preprocessing with the data, so the raw data is exported from Earth Engine.\n",
    "# The function takes an image, a list of strings with the selected feature bands in the image, the name of the response\n",
    "# band in this image, the number of samples/pixels the user wants, and the number of parititions to trim within\n",
    "\n",
    "def trim_data(image, selected_features, response_band, num_samples, num_partitions):\n",
    "    image = ee.Image(image)\n",
    "    selected_features = ee.List(selected_features)\n",
    "    response_band = ee.String(response_band)\n",
    "    num_samples = ee.Number(num_samples)\n",
    "    num_partitions = ee.Number(num_partitions)\n",
    "    \n",
    "    # Generate the list of percentile bounds for the requested number of partitions, and the names of the value bounds for the\n",
    "    # dictionary that will be generated from the percentile reducer used later on\n",
    "    percentiles = ee.List.sequence(0, 100, ee.Number(100).divide(num_partitions))\n",
    "    percentile_names = percentiles.map(lambda num: ee.Number(num).round().toInt().format(\"p%s\"))\n",
    "    \n",
    "    # Randomly sample pixels in the input image into a FeatureCollection containing only selected features and response\n",
    "    image_pixels = ee.Number(get_num_pixels(image))\n",
    "    inputsCollection = image.select(selected_features.add(response_band)).sample(numPixels=num_samples.min(image_pixels))\n",
    "    \n",
    "    # Find the values at the percentile bounds using the percentile reducer over the feature collection\n",
    "    response_percentiles = inputsCollection.reduceColumns(ee.Reducer.percentile(percentiles=percentiles, \\\n",
    "                                        outputNames=percentile_names, maxRaw=inputsCollection.size()), [response_band])\n",
    "    \n",
    "    # Create a list of percentile bounds for each partition\n",
    "    response_partitions = response_percentiles.values(percentile_names.remove('p100'))\\\n",
    "                                .zip(response_percentiles.values(percentile_names.remove('p0')))\n",
    "    \n",
    "    # Use the following mapped over the response_partitions list to partition the data by the requested number of partitions\n",
    "    def partition_data(partition_range):\n",
    "        partition_range = ee.List(partition_range)\n",
    "        return inputsCollection.filter(ee.Filter \\\n",
    "                            .rangeContains(response_band, partition_range.getNumber(0), partition_range.getNumber(1)))\n",
    "    \n",
    "    partitioned_data = response_partitions.map(partition_data)\n",
    "    \n",
    "    # The following function now trims the data in each partition individually for each feature to its 5-95 percentile only\n",
    "    def trim_partitions(partition):\n",
    "        partition = ee.FeatureCollection(partition)\n",
    "        feature_trimming_bounds = selected_features.map(lambda feature: ee.List([feature]) \\\n",
    "                     .cat(partition.reduceColumns(ee.Reducer.percentile([5, 95]), [feature]).values(['p5','p95'])))\n",
    "        def trimmer(current_feature, collection):\n",
    "            current_feature = ee.List(current_feature)\n",
    "            collection = ee.FeatureCollection(collection)\n",
    "            return collection.filter(ee.Filter.rangeContains( \\\n",
    "                        current_feature.getString(0), current_feature.getNumber(1), current_feature.getNumber(2)))\n",
    "        return feature_trimming_bounds.iterate(trimmer, partition)\n",
    "    \n",
    "    # Retrieve the trimmed data partitions and flatten the paritions into a single trimmed feature collection\n",
    "    trimmed_partitions = partitioned_data.map(trim_partitions)\n",
    "    trimmed_data = ee.FeatureCollection(trimmed_partitions).flatten()\n",
    "    \n",
    "    return trimmed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_nnet(inputs, keras_model):\n",
    "    activation_functions = {\n",
    "        \"elu\": elu,\n",
    "        \"softplus\": softplus,\n",
    "        \"softsign\": softsign,\n",
    "        \"relu\": relu,\n",
    "        \"tanh\": tanh,\n",
    "        \"sigmoid\": sigmoid}\n",
    "    \n",
    "    for layer in keras_model.layers:\n",
    "        layer_weights = layer.get_weights()\n",
    "        node_weights = layer_weights[0]\n",
    "        bias = layer_weights[1]\n",
    "        \n",
    "        inputs = inputs.dot(node_weights)+bias\n",
    "        \n",
    "        activation_function = layer.get_config()[\"activation\"]\n",
    "        if(activation_function != \"linear\"):\n",
    "            activation_function = activation_functions[activation_function]\n",
    "            inputs = activation_function(inputs)\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following function exports the keras model in a way that can be parsed into a FeatureCollection in Earth Engine and applied to images manually\n",
    "\n",
    "def export_nnet(keras_model, X):\n",
    "    nnet_data = []\n",
    "    headers = []\n",
    "    prev_layer_size = len(X.keys())\n",
    "    layer_num = 0\n",
    "    \n",
    "    for layer in keras_model.layers:\n",
    "        layer_info = layer.get_config()\n",
    "        num_nodes = layer_info[\"units\"]\n",
    "        activation_function = layer_info[\"activation\"]\n",
    "        layer_weights = layer.get_weights()[0]\n",
    "        layer_bias = layer.get_weights()[1]\n",
    "        headers = list(set(headers) | set([x for x in range((prev_layer_size+1)*num_nodes)]))\n",
    "        layer_data = [0, 0, layer_num, prev_layer_size, num_nodes, activation_function] \\\n",
    "                            + layer_weights.flatten().tolist() + layer_bias.tolist()\n",
    "        nnet_data.append(layer_data)\n",
    "        \n",
    "        prev_layer_size = num_nodes\n",
    "        layer_num += 1\n",
    "        \n",
    "    nnet_data.insert(0, [\"latitude\", \"longitude\", \"layer_num\", \"prev_layer_size\", \"num_nodes\", \"activation\"]+headers)\n",
    "    return nnet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test image using an uploaded asset for now\n",
    "testImage = ee.Image('users/kateharvey/i')\n",
    "# testImage = ee.Image('users/kateharvey/TestImage')\n",
    "\n",
    "# Change variable name here to match band name pattern (one of: Albedo, fAPAR, fCOVER, LAI, CCC, CWC, DASF)\n",
    "outputName = 'LAI'\n",
    "defaultBand = 'estimate'+outputName\n",
    "\n",
    "# order of bands:\n",
    "# 0-13: B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B10, B11, B12, B13\n",
    "# 23: date\n",
    "# 27-33: QC, estimateLAI, partition, networkID, errorLAI, partition_1, networkID_1\n",
    "inputImage = ee.Image(testImage.select(1,2,3,7,23,27,28,29,30,31,32,33))\n",
    "inputImage = inputImage.rename(['B2', 'B3', 'B4', 'B8', 'date', 'QC', 'estimateLAI', 'partition', 'networkID', 'errorLAI', 'partition_1', 'networkID_1'])\n",
    "inputImage = inputImage.addBands(inputImage.select('B3').divide(10000), overwrite=True)\n",
    "inputImage = inputImage.addBands(inputImage.select('B4').divide(10000), overwrite=True)\n",
    "\n",
    "# List the bands that we expect in the image (ensure number of bands in the list below matches the number bands in the input image)\n",
    "# This is the order of bands produced by running SL2P10 and exporting the resulting ImageCollection\n",
    "inputImage_bands = ee.List(['B2', 'B3', 'B4', 'B8', 'date', 'QC', 'estimateLAI', 'partition', 'networkID', 'errorLAI', 'partition_1', 'networkID_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegetation Indices Sources\n",
    "\n",
    "1. https://www.hindawi.com/journals/js/2017/1353691/tab1/\n",
    "2. https://www.hiphen-plant.com/blog/vegetation-indices/\n",
    "3. https://gisgeography.com/sentinel-2-bands-combinations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we define a list of strings representing the expressions for each vegetation index as a function of the bands in the input image\n",
    "# More vegetation indices can be defined, but the list CANNOT contain any two vegetation indices which are a linear combination of each\n",
    "# other or LARs will fail to select the requested number of variables\n",
    "\n",
    "# The formatting of the expression must be\n",
    "# \"<name of VI> = <expression with band names from inputImage_bands used as variables in the form b('<band name>')\"\n",
    "\n",
    "# Only include VIs that use 10 m bands (B2, B3, B4, B8)\n",
    "input_VI_definition = ee.List([\"RAW_B2  = b('B2')\",\n",
    "                               \"RAW_B3  = b('B3')\",\n",
    "                               \"RAW_B4  = b('B4')\",\n",
    "                               \"RAW_B8  = b('B8')\",\n",
    "                               \"GI      = b('B3')/b('B4')\",\n",
    "                             # \"RVI3    = b('B4')/b('B6')\",\n",
    "                             # \"SR3     = b('B5')/b('B4')\",\n",
    "                             # \"GM1     = b('B6')/b('B3')\",\n",
    "                             # \"GM2     = b('B6')/b('B5')\",\n",
    "                             # \"SR2     = b('B7')/b('B3')\",\n",
    "                             # \"PSSR    = b('B7')/b('B4')\",\n",
    "                               \"SGI     = b('B8')/b('B4')\",\n",
    "                             # \"MSI     = b('B11')/b('B7')\",\n",
    "                             # \"II      = b('B11')/b('B12')\",\n",
    "                               \"GVI     = (b('B8')/b('B3'))-1\",\n",
    "                             # \"PSRI    = (b('B4')-b('B3'))/b('B6')\",\n",
    "                               \"NDVI3   = ((b('B8')-b('B4'))/(b('B8')))+b('B4')\",\n",
    "                             # \"SR5     = 1/b('B5')\",\n",
    "                             # \"SR6     = b('B4')/(b('B3')*b('B5'))\",\n",
    "                             # \"SR7     = b('B8')/(b('B3')*b('B5'))\",\n",
    "                             # \"IPVI    = b('B7')/(b('B7')+b('B4'))\",\n",
    "                             # \"ARI     = (1/b('B3'))-(1/b('B5'))\",\n",
    "                             # \"ARI2    = b('B7')*((1/b('B3'))-(1/b('B5')))\",\n",
    "                               \"NDVI    = (b('B8')-b('B4'))/(b('B8')+b('B4'))\",\n",
    "                               \"GNDVI   = (b('B8')-b('B3'))/(b('B8')+b('B3'))\",\n",
    "                             # \"NDWI    = (b('B8')-b('B11'))/(b('B8')+b('B11'))\",\n",
    "                             # \"NDREVI  = (b('B8')-b('B5'))/(b('B8')+b('B5'))\",\n",
    "                               \"NDGI    = (b('B3')-b('B4'))/(b('B3')+b('B4'))\",\n",
    "                             # \"NDI1    = (b('B7')-b('B5'))/(b('B7')-b('B4'))\",\n",
    "                             # \"NDI2    = (b('B8')-b('B5'))/(b('B8')-b('B4'))\",\n",
    "                             # \"RENDVI  = (b('B6')-b('B5'))/(b('B6')+b('B5'))\",\n",
    "                             # \"OSAVI   = (1.16*(b('B7')-b('B4')))/(b('B7')+b('B4')+0.61)\",\n",
    "                             # \"NMDI    = (b('B8')-(b('B11')-b('B12')))/(b('B8')+(b('B11')-b('B12')))\",\n",
    "                             # \"HI      = ((b('B3')-b('B5'))/(b('B3')+b('B5')))-0.5*b('B5')\",\n",
    "                             # \"GVSP    = (-0.283*b('B3') - 0.66*b('B4') + 0.577*b('B6') + 0.388*b('B8'))/(0.433*b('B3') - 0.632*b('B4') + 0.586*b('B6') + 0.264*b('B8A'))\",\n",
    "                             # \"MCARI   = ((b('B5')-b('B4'))-0.2*(b('B5')-b('B3')))*(b('B5')/b('B4'))\",\n",
    "                             # \"TCARI   = 3*((b('B5')-b('B4'))-0.2*(b('B5')-b('B3'))*(b('B5')/b('B4')))\",\n",
    "                               \"EVI     = 2.5*((b('B8')-b('B4'))/(b('B8')+6*b('B4')-7.5*b('B3')+1))\",\n",
    "                               \"EVI2    = 2.5*((b('B8')-b('B4'))/(b('B8')+2.4*b('B4')+1))\",\n",
    "                               \"RDVI    = (b('B8')-b('B4'))/((b('B8')+b('B4'))**0.5)\",\n",
    "                               \"MSR     = ((b('B8')/b('B4'))-1)/((b('B8')/b('B4'))**0.5+1)\",\n",
    "                             # \"MSAVI   = 0.5*(2*b('B7')+1-((2*b('B7')+1)**2-8*(b('B7')-b('B4')))**0.5)\",\n",
    "                               \"MSAVI2  = 0.5*(2*b('B8')+1-((2*b('B8')+1)**2-8*(b('B8')-b('B4')))**0.5)\",\n",
    "                             # \"MCARI2  = (1.5*(2.5*(b('B7')-b('B4'))-1.3*(b('B7')-b('B3'))))/((((2*b('B7')+1)**2)-(6*b('B7')-5*(b('B4')**0.5))-0.5)**0.5)\",\n",
    "                             # \"MTVI2   = (1.5*(1.2*(b('B7')-b('B3'))-2.5*(b('B4')-b('B3'))))/(((2*b('B7')+1)**2-(6*b('B7')-5*b('B4'))-0.5)**0.5)\",\n",
    "                             # \"MSR2    = ((b('B7')/b('B4'))-1)/(((b('B7')/b('B4'))+1)**0.5)\",\n",
    "                               \"NLI     = ((b('B8')**2)-b('B4'))/((b('B8')**2)+b('B4'))\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following functions each input is recast to the expected data type as function parameter typing is not enforced in Earth Engine\n",
    "when defining functions, and later methods called on these parameters within the function must recognize the type of the parameter\n",
    "independently of other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputImage = format_image(inputImage, inputImage_bands, 'estimateLAI', input_VI_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_pixels = get_num_pixels(inputImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full image with bands: 'RAW_B2', 'RAW_B3', 'RAW_B4', 'RAW_B8', 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI', 'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI', 'B2', 'B3', 'B4', 'B8', 'date', 'QC', 'partition', 'networkID', 'errorLAI', 'partition_1', 'networkID_1', 'estimateLAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledImage = scale_image(inputImage, 'estimateLAI')\n",
    "# print(scaledImage.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bandNames = ['B2', 'B3', 'B4', 'B8', 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI',\n",
    "                   'GNDVI', 'NDGI', 'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "['MSR', 'GVI', 'B8', 'NDGI', 'EVI2']\n"
     ]
    }
   ],
   "source": [
    "select_features = ee_LARS(scaledImage, input_bandNames, 'estimateLAI', 5, 1000)\n",
    "print(select_features.getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcing 10 m band selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras to create a sequential model neural network which only has simple dense layers of the specified number of nodes\n",
    "bands = ee.List(['B2', 'B3', 'B4', 'B8'])\n",
    "trimmedCollection = trim_data(image=inputImage.updateMask(inputImage.select('estimateLAI').gt(0)),\n",
    "                                  selected_features=bands,\n",
    "                                  response_band='estimateLAI',\n",
    "                                  num_samples=50000,\n",
    "                                  num_partitions=10)\n",
    "exportDataTest = ee.batch.Export.table.toDrive(collection=trimmedCollection,\n",
    "                                           description=\"test_image_data_samples\",\n",
    "                                           fileFormat=\"CSV\")\n",
    "\n",
    "# Starting the export data task\n",
    "exportDataTest.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY\n",
      "RUNNING\n",
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "prev_task_status = ee.data.getTaskStatus(exportDataTest.id)[0][\"state\"]\n",
    "print(prev_task_status)\n",
    "while exportDataTest.active():\n",
    "    task_status = ee.data.getTaskStatus(exportDataTest.id)[0][\"state\"]\n",
    "    if(task_status != prev_task_status):\n",
    "        print(task_status)\n",
    "    prev_task_status = task_status\n",
    "    time.sleep(5)\n",
    "print(ee.data.getTaskStatus(exportDataTest.id)[0][\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 14:55:34.104425: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Reading the CSV file into dataframes\n",
    "trimmed_data_test = pd.read_csv('./gdrive/test_image_data_samples.csv')\n",
    "X_test = trimmed_data_test.drop(labels=['estimateLAI', 'system:index', '.geo'], axis=1)\n",
    "y_test = trimmed_data_test.estimateLAI\n",
    "\n",
    "# We preprocess the input features by standardizing them to a mean of 0 and a standard deviation of 1 for the neural network\n",
    "X_test = pd.DataFrame(skl.preprocessing.scale(X_test))\n",
    "\n",
    "LAI_model_test = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(5, input_shape=[len(X_test.keys())]),\n",
    "    tf.keras.layers.Dense(4, activation=\"softsign\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softsign\"),\n",
    "    tf.keras.layers.Dense(2, activation=\"softsign\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_model_test.compile(optimizer=tf.keras.optimizers.Nadam(), loss='mse', metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 14:55:36.711616: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "963/963 [==============================] - 3s 2ms/step - loss: 7691627.2318 - mse: 7691627.2318 - mae: 2614.9345\n",
      "Epoch 2/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 7668140.8008 - mse: 7668140.8008 - mae: 2610.9341: 0s - loss: 7669186.2052 - mse: 7669186.2052 - mae: 2611.51 - ETA: 0s - loss: 7669188.9047 - mse:\n",
      "Epoch 3/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 7646521.5420 - mse: 7646521.5420 - mae: 2605.3898\n",
      "Epoch 4/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7587353.1758 - mse: 7587353.1758 - mae: 2593.1982\n",
      "Epoch 5/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7587262.8636 - mse: 7587262.8636 - mae: 2594.0293\n",
      "Epoch 6/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7567957.3496 - mse: 7567957.3496 - mae: 2590.2999\n",
      "Epoch 7/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7560264.6852 - mse: 7560264.6852 - mae: 2587.7352\n",
      "Epoch 8/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7609812.8065 - mse: 7609812.8065 - mae: 2597.3683\n",
      "Epoch 9/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7526071.9741 - mse: 7526071.9741 - mae: 2582.7938\n",
      "Epoch 10/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7513677.6291 - mse: 7513677.6291 - mae: 2582.3828\n",
      "Epoch 11/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7562269.5721 - mse: 7562269.5721 - mae: 2589.1753\n",
      "Epoch 12/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7485592.7941 - mse: 7485592.7941 - mae: 2573.7223\n",
      "Epoch 13/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7448145.0690 - mse: 7448145.0690 - mae: 2567.0912\n",
      "Epoch 14/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7514842.5685 - mse: 7514842.5685 - mae: 2578.5103\n",
      "Epoch 15/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7478269.3600 - mse: 7478269.3600 - mae: 2571.8806\n",
      "Epoch 16/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7427364.2759 - mse: 7427364.2759 - mae: 2561.2597: 0s - loss: 7427222.9669 - mse: 7427222.9669 - mae: 2561.21\n",
      "Epoch 17/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7451685.8117 - mse: 7451685.8117 - mae: 2567.2661\n",
      "Epoch 18/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7440490.7106 - mse: 7440490.7106 - mae: 2565.2882: 1s -\n",
      "Epoch 19/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7396768.1649 - mse: 7396768.1649 - mae: 2558.1806\n",
      "Epoch 20/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7388672.4953 - mse: 7388672.4953 - mae: 2553.8495\n",
      "Epoch 21/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7413451.4196 - mse: 7413451.4196 - mae: 2558.3191: 0s - loss: 7430867.5578 - ms\n",
      "Epoch 22/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7377813.9212 - mse: 7377813.9212 - mae: 2552.9699\n",
      "Epoch 23/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7361917.6240 - mse: 7361917.6240 - mae: 2548.5084\n",
      "Epoch 24/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7272889.9414 - mse: 7272889.9414 - mae: 2534.4196\n",
      "Epoch 25/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7358124.0000 - mse: 7358124.0000 - mae: 2547.8337\n",
      "Epoch 26/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7299516.5788 - mse: 7299516.5788 - mae: 2537.7008\n",
      "Epoch 27/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7300139.7588 - mse: 7300139.7588 - mae: 2536.6947\n",
      "Epoch 28/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 7238975.0742 - mse: 7238975.0742 - mae: 2526.3680\n",
      "Epoch 29/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7294395.4538 - mse: 7294395.4538 - mae: 2534.8015\n",
      "Epoch 30/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 7221077.6022 - mse: 7221077.6022 - mae: 2522.0505\n",
      "Epoch 31/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 7220978.6655 - mse: 7220978.6655 - mae: 2521.7694\n",
      "Epoch 32/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7257280.3823 - mse: 7257280.3823 - mae: 2529.7668\n",
      "Epoch 33/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7164234.5877 - mse: 7164234.5877 - mae: 2510.9060\n",
      "Epoch 34/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7156314.7339 - mse: 7156314.7339 - mae: 2509.4585: 0s - loss: 7142341.3480 - mse: 71423\n",
      "Epoch 35/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7152198.3579 - mse: 7152198.3579 - mae: 2507.8869\n",
      "Epoch 36/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7184359.8195 - mse: 7184359.8195 - mae: 2516.2377\n",
      "Epoch 37/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7156908.2702 - mse: 7156908.2702 - mae: 2508.6225\n",
      "Epoch 38/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 7142931.9570 - mse: 7142931.9570 - mae: 2505.8474\n",
      "Epoch 39/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 7068852.6457 - mse: 7068852.6457 - mae: 2493.2763\n",
      "Epoch 40/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7057932.7090 - mse: 7057932.7090 - mae: 2489.1322\n",
      "Epoch 41/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7115488.7210 - mse: 7115488.7210 - mae: 2500.2223\n",
      "Epoch 42/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7116283.8076 - mse: 7116283.8076 - mae: 2500.3220\n",
      "Epoch 43/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7006811.4870 - mse: 7006811.4870 - mae: 2478.2865: 0s - loss: 7002630.8619 - mse: 7002630.8619 \n",
      "Epoch 44/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7015490.3377 - mse: 7015490.3377 - mae: 2480.9238\n",
      "Epoch 45/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7013968.7614 - mse: 7013968.7614 - mae: 2479.7727\n",
      "Epoch 46/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7029471.3320 - mse: 7029471.3320 - mae: 2484.1332\n",
      "Epoch 47/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 7030851.5669 - mse: 7030851.5669 - mae: 2485.3907: 0s - loss: 7034007.9327 - mse: 7034007.9327 - ma\n",
      "Epoch 48/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6953501.3600 - mse: 6953501.3600 - mae: 2467.4538: 0s - loss: 6945079.1968 - mse: 6\n",
      "Epoch 49/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6961179.4440 - mse: 6961179.4440 - mae: 2471.2489\n",
      "Epoch 50/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6988650.7049 - mse: 6988650.7049 - mae: 2475.2874\n",
      "Epoch 51/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6951873.9751 - mse: 6951873.9751 - mae: 2470.8843\n",
      "Epoch 52/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6875497.2412 - mse: 6875497.2412 - mae: 2454.4707\n",
      "Epoch 53/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6921739.7303 - mse: 6921739.7303 - mae: 2464.5196\n",
      "Epoch 54/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6869401.3345 - mse: 6869401.3345 - mae: 2452.3458\n",
      "Epoch 55/100\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 6896013.4668 - mse: 6896013.4668 - mae: 2456.1521\n",
      "Epoch 56/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 6858369.5337 - mse: 6858369.5337 - mae: 2449.5149\n",
      "Epoch 57/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6845893.0737 - mse: 6845893.0737 - mae: 2447.1432\n",
      "Epoch 58/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 6851691.0353 - mse: 6851691.0353 - mae: 2448.3323\n",
      "Epoch 59/100\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 6815873.5534 - mse: 6815873.5534 - mae: 2439.8293\n",
      "Epoch 60/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 6800949.4315 - mse: 6800949.4315 - mae: 2441.3641\n",
      "Epoch 61/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6813594.0633 - mse: 6813594.0633 - mae: 2440.1322 ETA: 0s - loss: 6817415.4887 - mse: 6817415.4887 - ma\n",
      "Epoch 62/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6781144.3034 - mse: 6781144.3034 - mae: 2433.4635\n",
      "Epoch 63/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6781904.1141 - mse: 6781904.1141 - mae: 2433.4697\n",
      "Epoch 64/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6747245.2490 - mse: 6747245.2490 - mae: 2427.9679\n",
      "Epoch 65/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6747038.0711 - mse: 6747038.0711 - mae: 2426.5809\n",
      "Epoch 66/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6749551.7318 - mse: 6749551.7318 - mae: 2428.4523\n",
      "Epoch 67/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6716808.1172 - mse: 6716808.1172 - mae: 2421.8921\n",
      "Epoch 68/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6709578.7666 - mse: 6709578.7666 - mae: 2417.9841\n",
      "Epoch 69/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6714698.9258 - mse: 6714698.9258 - mae: 2423.1660: 0s - loss: 6720600.0810 - mse: 6720600.0810 \n",
      "Epoch 70/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6654870.5446 - mse: 6654870.5446 - mae: 2409.0403: 1s - los\n",
      "Epoch 71/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 6648659.7085 - mse: 6648659.7085 - mae: 2407.4397\n",
      "Epoch 72/100\n",
      "963/963 [==============================] - 2s 3ms/step - loss: 6615767.4559 - mse: 6615767.4559 - mae: 2399.4978\n",
      "Epoch 73/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6620882.9289 - mse: 6620882.9289 - mae: 2401.9371\n",
      "Epoch 74/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6634118.9155 - mse: 6634118.9155 - mae: 2403.9979\n",
      "Epoch 75/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6608748.3449 - mse: 6608748.3449 - mae: 2400.3546\n",
      "Epoch 76/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6613558.1592 - mse: 6613558.1592 - mae: 2403.2985\n",
      "Epoch 77/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6563477.9362 - mse: 6563477.9362 - mae: 2388.9677\n",
      "Epoch 78/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6567615.7360 - mse: 6567615.7360 - mae: 2390.3609\n",
      "Epoch 79/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6578286.1639 - mse: 6578286.1639 - mae: 2391.8962\n",
      "Epoch 80/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6531145.6758 - mse: 6531145.6758 - mae: 2384.2637\n",
      "Epoch 81/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6494086.9834 - mse: 6494086.9834 - mae: 2375.6493\n",
      "Epoch 82/100\n",
      "963/963 [==============================] - 3s 3ms/step - loss: 6521183.0135 - mse: 6521183.0135 - mae: 2380.0995\n",
      "Epoch 83/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6516555.1540 - mse: 6516555.1540 - mae: 2381.3150\n",
      "Epoch 84/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6443031.4180 - mse: 6443031.4180 - mae: 2362.2932\n",
      "Epoch 85/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6460965.9035 - mse: 6460965.9035 - mae: 2368.0848\n",
      "Epoch 86/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6418340.4336 - mse: 6418340.4336 - mae: 2358.8874\n",
      "Epoch 87/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6430369.1385 - mse: 6430369.1385 - mae: 2360.7637\n",
      "Epoch 88/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6409635.7116 - mse: 6409635.7116 - mae: 2357.3035\n",
      "Epoch 89/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6395918.9907 - mse: 6395918.9907 - mae: 2353.9842: 0s - loss: 6392874.1648 - mse: 6392874\n",
      "Epoch 90/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6389462.9103 - mse: 6389462.9103 - mae: 2353.5521\n",
      "Epoch 91/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6360535.9881 - mse: 6360535.9881 - mae: 2347.1607: 1s - loss: 6343661.7626 - mse: 63436 - ETA: 0s - loss: 6353355.2168 - mse:\n",
      "Epoch 92/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6339701.4984 - mse: 6339701.4984 - mae: 2340.1370\n",
      "Epoch 93/100\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 6319092.3843 - mse: 6319092.3843 - mae: 2340.0700\n",
      "Epoch 94/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6329395.1618 - mse: 6329395.1618 - mae: 2339.1235\n",
      "Epoch 95/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6333456.4481 - mse: 6333456.4481 - mae: 2342.3972\n",
      "Epoch 96/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6300682.0524 - mse: 6300682.0524 - mae: 2334.4149\n",
      "Epoch 97/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6311375.2412 - mse: 6311375.2412 - mae: 2335.4761\n",
      "Epoch 98/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6297945.3994 - mse: 6297945.3994 - mae: 2334.0465\n",
      "Epoch 99/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6255625.2329 - mse: 6255625.2329 - mae: 2326.1448\n",
      "Epoch 100/100\n",
      "963/963 [==============================] - 2s 2ms/step - loss: 6240236.5415 - mse: 6240236.5415 - mae: 2321.4903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15b8ce730>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model to our trimmed data\n",
    "LAI_model_test.fit(x=X_test.to_numpy(), y=y_test.to_numpy(), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting our own input data to evaluate the performance (for now)\n",
    "LAI_predictions_test = pd.Series(LAI_model_test.predict(X_test.to_numpy()).flatten())\n",
    "\n",
    "# Prepare data to display as a scatterplot\n",
    "xy_tf_LAI_test = np.vstack([y_test, LAI_predictions_test])\n",
    "z_tf_LAI_test = scipy.stats.gaussian_kde(xy_tf_LAI_test)(xy_tf_LAI_test)\n",
    "\n",
    "idx_tf_LAI_test = z_tf_LAI_test.argsort()\n",
    "x_tf_LAI_test = y_test[idx_tf_LAI_test]\n",
    "y_tf_LAI_test = LAI_predictions_test[idx_tf_LAI_test]\n",
    "z_tf_LAI_test = z_tf_LAI_test[idx_tf_LAI_test]\n",
    "\n",
    "rmse_tf_LAI_test = skl.metrics.mean_squared_error(x_tf_LAI_test, y_tf_LAI_test, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3\n",
      "0      0.669287  0.588508  0.767708 -1.132185\n",
      "1      1.994071  1.435797  2.944549 -2.340043\n",
      "2      2.124073  1.503131  2.850829 -2.564114\n",
      "3      0.817861  0.874678  1.044605 -1.194500\n",
      "4      1.133581  1.256239  1.649520 -1.596236\n",
      "...         ...       ...       ...       ...\n",
      "30805 -0.092154 -0.112890 -0.293023  1.002449\n",
      "30806 -0.562638 -0.500062 -0.667900  1.233149\n",
      "30807 -0.630734 -0.556174 -0.663640  0.702805\n",
      "30808 -0.605972 -0.612286 -0.697719  0.977258\n",
      "30809 -0.488351 -0.528118 -0.586960  1.055484\n",
      "\n",
      "[30810 rows x 4 columns]\n",
      "0        1376.0\n",
      "1         134.0\n",
      "2         174.0\n",
      "3        1212.0\n",
      "4        1218.0\n",
      "          ...  \n",
      "30805    4187.0\n",
      "30806    4001.0\n",
      "30807    3879.0\n",
      "30808    3896.0\n",
      "30809    4069.0\n",
      "Name: estimateLAI, Length: 30810, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "a_tf_LAI_test = np.linspace(0, 3, 1000)\n",
    "fig_test, ax_test = plt.subplots(1, 1, figsize=(10,10))\n",
    "\n",
    "ax_test.scatter(x_tf_LAI_test, y_tf_LAI_test, c=z_tf_LAI_test)\n",
    "ax_test.plot(a_tf_LAI_test, a_tf_LAI_test, c='r')\n",
    "ax_test.title.set_text('LASSO LARS (ALL BANDS) - RMSE: {}'.format(rmse_tf_LAI_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the trimmed data is processed then in a neural network created using tensorflow to find nonlinear relationships between the predictor and the response. Earth Engine does not have this functionality (for free) to generate neural network based models.\n",
    "\n",
    "Here we also see how the server side in the Earth Engine API is completely separate from the client side on the local machine. We need\n",
    "to export our trimmed data as a CSV to a google drive which is synced into the \"gdrive\" folder in our local machine using the \n",
    "Backup and Sync software or using google-drive-ocamlfuse on Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early attempts to use the output of the kerasModel.get_weights() to apply the neural network weights manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LAI_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu = np.vectorize(alr.elu)\n",
    "softplus = np.vectorize(alr.softplus)\n",
    "softsign = np.vectorize(alr.softsign)\n",
    "relu = np.vectorize(alr.relu)\n",
    "tanh = np.vectorize(alr.tanh)\n",
    "sigmoid = np.vectorize(alr.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 4802\n",
    "inputs = X.iloc[row, :].to_numpy()\n",
    "print(LAI_model.predict(inputs.reshape((-1,5)))[0][0])\n",
    "print(alr.apply_nnet(inputs, LAI_model)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the neural network to a CSV file to be uploaded to the server side on Google Earth Engine\n",
    "export_data = alr.export_nnet(LAI_model, X)\n",
    "with open('nnet.csv', 'w', newline='') as csvfile:\n",
    "    nnet_writer = csv.writer(csvfile)\n",
    "    for layerdata in export_data:\n",
    "        nnet_writer.writerow(layerdata)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Currently, I have not found a way to automatically upload a file as an asset into Google Earth Engine from a local script. There is a command in the Earth Engine command line interface that allows you to upload an asset from cloud storage, although cloud storage is not free. There may be a workaround way to upload the file from Google Drive directly to Earth Engine, but even that would use the Google Drive API which is a part of Google Cloud, so it will not be free. It seems the network must be uploaded manually for now."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LAI_FAPAR_FCOVER_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
